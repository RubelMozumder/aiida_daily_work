{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "moved-syndicate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "profil name: default\n"
     ]
    }
   ],
   "source": [
    "# Load the aiida profile\n",
    "from aiida import load_profile\n",
    "aiida_profile = load_profile()\n",
    "print(f'profil name: {aiida_profile.name}')\n",
    "\n",
    "## Loading the some require packages and module\n",
    "from aiida_kkr.workflows import (combine_imps_wc, kkr_flex_wc,\n",
    "                                 kkr_imp_wc, kkr_imp_sub_wc\n",
    "                                )\n",
    "from aiida_kkr.calculations import KkrCalculation, KkrimpCalculation\n",
    "from aiida.orm import (Group, load_group, load_node, Dict, Code, QueryBuilder, WorkChainNode\n",
    "                       , CalcJobNode)\n",
    "from aiida.transports import transport\n",
    "from aiida_kkr.tools import kkrparams\n",
    "from aiida.engine import submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "molecular-occasion",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# An attemp to run a bunch of wc of the double impurity from two groups of single impurity wc or \n",
    "# two list of the single impurity wc\n",
    "\n",
    "def submit_double_imp_wc_bunch( si_imp_list1, si_imp_list2, kkr_code, kkr_imp_code, builder_options,  \n",
    "                                succ_group_label, succ_group_descr, offset_imp2={'index':1}, \n",
    "                                max_submission=20, setting=None, gf_host_remote=None,\n",
    "                                scf_wf_parameters=None, params_kkr_overwrite=None, dry_run= True,\n",
    "                                max_fail_wc = 10, debug= False\n",
    "                              ):\n",
    "    import time as t\n",
    "    \"\"\"\n",
    "    params: \n",
    "        : Two lists : (si_imp_list1, si_imp_list2) : Two single imp wc list for combination\n",
    "        : kkr_imp_code : kkr_imp code for kkr_imp_sub_wc\n",
    "        : kkr_code : kkr_code for kkr_flex_wc\n",
    "        : Builder_options Dict : computer settings.\n",
    "        : succ_group_label str : Create a group by this name if it does not exist in the DB.\n",
    "        : succ_group_descr str : Set the created group description either it exists or not, it not a new group\n",
    "                            will be created.\n",
    "        : offset_imp2 : {'index':1}; for 0,1,.. same unit cell or nearest unit cell and so on.\n",
    "        : max_submissioin : This is the max number of wc for submission runing at the same time.\n",
    "        : Settings Dict ot dict : Parameter settings.\n",
    "        : gf_host_remote : RemoteData : Remote kkr flex files\n",
    "        : scf_wf_parameters : wf_parameters for the scf.wf_parameters\n",
    "        : params_kkr_overwrite : wf_parameters for host.wf_parameters\n",
    "        : dry_run : Just for check the code\n",
    "        : max_fail_wc = stop submission if failed calculation is equal or more than max_fail_wc\n",
    "        : debug : Also for check the coding\n",
    "    \"\"\"    \n",
    "    # Create group or load existing group\n",
    "    succ_group = group_not_exist_create(group_label=succ_group_label, group_descr=succ_group_descr)\n",
    "    fail_group = group_not_exist_create(group_label=succ_group_label+'_fail', group_descr=succ_group_descr+'_fail')\n",
    "    all_submission_group = group_not_exist_create(group_label=succ_group_label+'_all_submit', group_descr=succ_group_descr+'_all_submit')    \n",
    "#     test_data and prepare for further\n",
    "    if not isinstance(si_imp_list1, list):\n",
    "        print('The given impurity wc list 1 is not the list type.')\n",
    "        return print('Please provide single_imp1_list.')\n",
    "    if not isinstance(si_imp_list2, list):\n",
    "        print('The given impurity wc list 2 is not the list type.')\n",
    "        return print('Please provide single_imp2_list.')\n",
    "    \n",
    "    all_combination_dict = {}\n",
    "    all_success_dict = {}\n",
    "    all_submission_dict = {}\n",
    "    all_resedue_dict = {}\n",
    "    \n",
    "    tot_wc_num = 0\n",
    "    all_submission_num = 0\n",
    "    all_success_num = 0\n",
    "    all_failed_num = 0\n",
    "    \n",
    "    if isinstance(offset_imp2, dict):\n",
    "        offset_imp2 = Dict(dict=offset_imp2)\n",
    "    \n",
    "    # Here to create all the possible dict\n",
    "    for i in si_imp_list1[:]:\n",
    "        for j in si_imp_list2[:]:\n",
    "            node_truple = (i, j)\n",
    "            pk_truple = (i.pk, j.pk)\n",
    "#            imp1_info = i.inputs.impurity_info.get_dict()\n",
    "#           imp2_info = j.inputs.impurity_info.get_dict()\n",
    "            offset_index= offset_imp2.get_dict()['index']\n",
    "#            ilayer1 = str(imp1_info['ilayer_center'])\n",
    "#            ilayer2 = str(imp2_info['ilayer_center'])\n",
    "#            label= i.label.split(':')[0] + ':'+ j.label+ '_il_' + ilayer1+'_il_' + ilayer2+'_offset_'+str(offset_index)        \n",
    "            label= node_label_creation(node1=i, node2=j, offset_index= offset_index)\n",
    "            all_combination_dict[tot_wc_num] = {'node_truple' : node_truple,\n",
    "                                                'pk_truple' : pk_truple,\n",
    "                                                'label' : label,\n",
    "                                                'submission': None}\n",
    "            tot_wc_num +=1\n",
    "    \n",
    "    all_resedue_dict = all_combination_dict.copy()\n",
    "    \n",
    "    # To start the submission process, save the success nodes and failed nodes into the corresponding\n",
    "    # group\n",
    "    N=0\n",
    "    for key, val in all_combination_dict.items():\n",
    "\n",
    "        imp1_wc_node = all_combination_dict[key]['node_truple'][0]\n",
    "        imp2_wc_node = all_combination_dict[key]['node_truple'][1]\n",
    "        label = all_combination_dict[key]['label']\n",
    "        \n",
    "## Check the wc exists in the all_submission_group if yes take them into the success of failed group. If still running 'still_running= True' \n",
    "        stil_running= False\n",
    "        wc_pre_submit, pre_submit_wc_pk= check_wc_exist_in_group(all_submission_group, wc_label=label)\n",
    "        \n",
    "        if wc_pre_submit:\n",
    "            pre_submit_node= load_node(pre_submit_wc_pk)\n",
    "            all_submission_num += 1\n",
    "            if pre_submit_node.is_finished == True:\n",
    "                if pre_submit_node.exit_status == 0 : \n",
    "                    all_success_num +=1\n",
    "                    print(f'Node pk: {pre_submit_wc_pk} is already submited and successfully finished. Adding to the group: {succ_group}') \n",
    "\n",
    "                    succ_group.add_nodes(pre_submit_node)\n",
    "                    all_submission_group.remove_nodes(pre_submit_node)\n",
    "\n",
    "                    del_node(node_pks=[pre_submit_node.pk], dry_run=False, verbosity=0, debug=False, only_remote_dir=True)\n",
    "                else:\n",
    "                    all_failed_num += 1\n",
    "                    print(f'Node pk: {pre_submit_wc_pk} is already submited and failed. Adding to the group: {fail_group}')\n",
    "                    fail_group.add_nodes(pre_submit_node)\n",
    "                    all_submission_group.remove_nodes(pre_submit_node)\n",
    "\n",
    "            elif pre_submit_node.is_excepted:\n",
    "                all_failed_num += 1\n",
    "                print(f'Node pk: {pre_submit_wc_pk} is already submited and failed. Adding to the group: {fail_group}')\n",
    "                fail_group.add_nodes(pre_submit_node)\n",
    "                all_submission_group.remove_nodes(pre_submit_node)\n",
    "    \n",
    "## ---------------------------        \n",
    "        \n",
    "        wc_pre_exist, _ = check_wc_exist_in_group(succ_group, wc_label=label)\n",
    "        wc_pre_fail, _ = check_wc_exist_in_group(fail_group, wc_label=label)\n",
    " \n",
    "        \n",
    "        \n",
    "                \n",
    "        \n",
    "        if wc_pre_exist or wc_pre_fail or wc_pre_submit:\n",
    "#             print('Already one wc named as {} is exist in this group'.format(label))\n",
    "            N += 1\n",
    "            # To reduce the tot_wc_num as they are either failed or sucessfully finished earlier\n",
    "            tot_wc_num -= 1\n",
    "            continue\n",
    "        else:\n",
    "            print('val : ', val)\n",
    "        \n",
    "        if all_failed_num == max_fail_wc:\n",
    "            print('This is for loop break, as all_failed_num == max_fail_wc')\n",
    "            break\n",
    "        \n",
    "        if debug: \n",
    "            print('imp1_pk for submission', imp1_wc_node.pk)\n",
    "            print('imp2_pk for submission', imp2_wc_node.pk)\n",
    "\n",
    "\n",
    "        if not dry_run: \n",
    "            submission = submit_combine_imp(imp1_wc_node, imp2_wc_node, kkr_code, kkr_imp_code, offset_imp2,\n",
    "                                            options, settings, dry_run, label, gf_host_remote,\n",
    "                                            scf_wf_parameters, params_kkr_overwrite )\n",
    "            t.sleep(10)\n",
    "            print(f\"THe submitted combine Imps is : {submission.pk}\")\n",
    "            all_submission_dict[key] = all_combination_dict[key].copy()\n",
    "            all_submission_dict[key]['submission'] = submission\n",
    "            all_submission_num += 1\n",
    "            all_submission_group.add_nodes(submission)\n",
    "        while((all_submission_num - all_success_num - all_failed_num >= max_submission) or \n",
    "                (tot_wc_num - all_submission_num == 0) or (all_failed_num == max_fail_wc)):\n",
    "                t.sleep(60*2)\n",
    "                \n",
    "#----------------------------------------------------------------------------------\n",
    "\n",
    "                pop_list = []\n",
    "                for submit_key in all_submission_dict.keys():\n",
    "                    submission = all_submission_dict[submit_key]['submission']\n",
    "                    if submission.is_finished == True:\n",
    "                        if submission.exit_status == 0 : \n",
    "                            all_success_num +=1\n",
    "                            print(f'all_success_num : {all_success_num}, and last pk : {submission.pk}') \n",
    "                            all_success_dict[submit_key] = all_combination_dict[submit_key].copy()\n",
    "                            all_resedue_dict.pop(submit_key)\n",
    "                        \n",
    "                            succ_group.add_nodes(submission)\n",
    "                            all_submission_group.remove_nodes(submission)\n",
    "                            pop_list.append(submit_key)\n",
    "                            \n",
    "                            del_node(node_pks=[submission.pk], dry_run=False, verbosity=0, debug=False, only_remote_dir=True)\n",
    "                        else:\n",
    "                            all_failed_num += 1\n",
    "                            print(f'all_failed_num : {all_failed_num}, last pk : {submission.pk}')\n",
    "                            fail_group.add_nodes(submission)\n",
    "                            all_submission_group.remove_nodes(submission)\n",
    "                            pop_list.append(submit_key)\n",
    "                            \n",
    "                    elif submission.is_excepted:\n",
    "                        all_failed_num += 1\n",
    "                        print(f'all_excepted_num : {all_failed_num}, last pk : {submission.pk}')\n",
    "                        fail_group.add_nodes(submission)\n",
    "                        all_submission_group.remove_nodes(submission)\n",
    "                        \n",
    "                        pop_list.append(submit_key)\n",
    "                        \n",
    "#---------------------------------------------------------------------------------\n",
    "                garbage = [all_submission_dict.pop(pop_key) for pop_key in pop_list[:]]\n",
    "                    \n",
    "                if len(all_submission_dict) == 0 :\n",
    "                    print('This is while loop break, as all_submission_dict.')\n",
    "                    break\n",
    "                if all_failed_num == max_fail_wc :\n",
    "                    if (all_submission_num - all_success_num - all_failed_num) == 0 :\n",
    "                        print('This while loop break, as (all_submission_num - all_success_num - all_failed_num) == 0')\n",
    "                        break\n",
    "                \n",
    "    print('total failed or sucsess wc : ', N)\n",
    "#     return all_success_dict, all_resedue_dict\n",
    "\n",
    "def submit_combine_imp(imp1_wc_node, imp2_wc_node, kkr_code, kkr_imp_code, offset_imp2, options, \n",
    "                       settings, dry_run, label, gf_host_remote=None,\n",
    "                       scf_wf_parameters = None, params_kkr_overwrite=None):\n",
    "    \n",
    "    from aiida_kkr.workflows import combine_imps_wc, kkr_imp_sub_wc, kkr_flex_wc\n",
    "    from aiida.orm import Dict\n",
    "    imp1_output = imp1_wc_node.outputs.workflow_info\n",
    "    imp2_output = imp2_wc_node.outputs.workflow_info\n",
    "    if scf_wf_parameters==None:\n",
    "        sub_wc1_node = imp1_wc_node.get_outgoing(node_class=kkr_imp_sub_wc).first().node\n",
    "        scf_wf_parameters = sub_wc1_node.inputs.wf_parameters\n",
    "    \n",
    "    if gf_host_remote==None:\n",
    "        if params_kkr_overwrite==None:\n",
    "            ##TODO: In case the gf_remote is from the parent calc: Another possible way to do it as, from inputs remote_gf->kkr_flex_wc \n",
    "            sub_gf_write_out = imp1_wc_node.get_outgoing(node_class=kkr_flex_wc).first().node\n",
    "                \n",
    "            params_kkr_overwrite = sub_gf_write_out.inputs.params_kkr_overwrite\n",
    "    if settings==None:\n",
    "        settings = imp1_wc_node.inputs.wf_parameters_overwrite\n",
    "    if label==None:\n",
    "        label = 'pk' + str(imp1_wc_node.pk)+':'+ str(imp1_wc_node.pk)\n",
    "    \n",
    "    builder = combine_imps_wc.get_builder()\n",
    "    builder.impurity1_output_node = imp1_output\n",
    "    builder.impurity2_output_node = imp2_output\n",
    "    if isinstance(offset_imp2, dict):\n",
    "        builder.offset_imp2 = Dict(dict=offset_imp2)\n",
    "    else:\n",
    "        builder.offset_imp2 = offset_imp2\n",
    "    \n",
    "    builder.scf.kkrimp = kkr_imp_code\n",
    "    builder.scf.options = options\n",
    "    builder.scf.wf_parameters = scf_wf_parameters\n",
    "    \n",
    "    if gf_host_remote==None:\n",
    "        builder.host_gf.kkr = kkr_code\n",
    "        builder.host_gf.options = options\n",
    "        builder.host_gf.params_kkr_overwrite = params_kkr_overwrite #host_gf.inputs.wf_parameters\n",
    "    else:\n",
    "        builder.gf_host_remote = gf_host_remote\n",
    "    if settings!=None:\n",
    "        if isinstance(settings, dict):\n",
    "            builder.wf_parameters_overwrite = Dict(dict=settings)\n",
    "        else:\n",
    "            builder.wf_parameters_overwrite = settings\n",
    "            \n",
    "    builder.metadata.label = label\n",
    "    if not dry_run:\n",
    "        submission = submit(builder)\n",
    "        return submission\n",
    "    else:\n",
    "        msg = ' This is dry_run. '\n",
    "        return msg\n",
    "    \n",
    "# This function create perfect node label by combining two single-single workflow \n",
    "# or for two sigle-double workflow. The first two inputs node1 and node2 are aiida nodes \n",
    "# and offset_index is integer \n",
    "def node_label_creation(node1, node2, offset_index):\n",
    "    OtherLabel1= node1.label\n",
    "    OtherLabel2= node2.label\n",
    "    ilSeparator= '_il_'\n",
    "    offSeparator= '_Off_'\n",
    "\n",
    "    ## To collect the host label\n",
    "    LabComs1= OtherLabel1.split(ilSeparator)\n",
    "    LabComs1= LabComs1[0].split(':')\n",
    "    host_label= LabComs1[-1]\n",
    "    ## Combining imps\n",
    "    LabComs1= OtherLabel1.split(host_label)\n",
    "    LabComs2= OtherLabel2.split(host_label)\n",
    "    Combine_imps= LabComs1[0]+LabComs2[0]\n",
    "    ## ilayers\n",
    "    LabComs1= OtherLabel1.split(ilSeparator)\n",
    "    LabComs2= OtherLabel2.split(ilSeparator)\n",
    "    Combine_il= LabComs1[1].split(offSeparator)[0] + '_' + LabComs2[1].split(offSeparator)[0]\n",
    "    ## offset settings part part\n",
    "    if offSeparator in OtherLabel1:\n",
    "        LabComs1= OtherLabel1.split(offSeparator)\n",
    "        LabComs1= ''+LabComs1[-1]\n",
    "    else:\n",
    "        LabComs1= ''\n",
    "    if offSeparator in OtherLabel2:\n",
    "        LabComs2= OtherLabel2.split(offSeparator)\n",
    "        LabComs2= LabComs2[-1].split('_')\n",
    "        LabComs2= [ str(int(i)+offset_index) for i in LabComs2]\n",
    "        rubish= ''\n",
    "        for i in LabComs2:\n",
    "            rubish += (i + '_')\n",
    "        rubish += 'RemovingTheLastHypeh'\n",
    "        rubish= rubish.replace('_RemovingTheLastHypeh','') \n",
    "        print(rubish)\n",
    "        LabComs2= str((0+offset_index))+ '_' +rubish \n",
    "    else:\n",
    "        LabComs2= str(0+offset_index)\n",
    "    if LabComs1 != '':\n",
    "        Combine_off= LabComs1 + '_'+ LabComs2\n",
    "    elif LabComs1 == '':\n",
    "        Combine_off= LabComs2\n",
    "    CombineLabel= Combine_imps+ host_label + ilSeparator + Combine_il + offSeparator + Combine_off\n",
    "#    print(f'New Label: {CombineLabel}, from the old labels {node1.label} and {node2.label}')\n",
    "    return CombineLabel\n",
    "        \n",
    "def group_not_exist_create(group_label, group_descr=None):\n",
    "    from aiida.orm import load_group, Group\n",
    "    \"\"\"\n",
    "        Check the group exist either must create\n",
    "    \"\"\"\n",
    "    if group_descr==None:\n",
    "        group_descr='No Description is added'\n",
    "    \n",
    "    try:\n",
    "         group = load_group(group_label)\n",
    "    except:\n",
    "        print('Group named {} is not exist but is being created .'.format(group_label))\n",
    "        group = Group(label=group_label, description=group_descr)\n",
    "        group.store()\n",
    "        print('Newly created group pk {}'.format(group.pk))\n",
    "    return group\n",
    "\n",
    "\n",
    "def check_wc_exist_in_group(group, wc_label=None, wc_pk=None):\n",
    "    nodes_list = list(group.nodes)\n",
    "    nodes_label = [i.label for i in nodes_list[:]]\n",
    "    nodes_pk = [i.pk for i in nodes_list[:]]\n",
    "    \n",
    "    if wc_label in nodes_label:\n",
    "        print('node_label-{} is exist in the group-{}.'.format(wc_label, group.label))\n",
    "        index= nodes_label.index(wc_label)\n",
    "        wc_pk= nodes_pk[index]\n",
    "        return True, wc_pk\n",
    "              \n",
    "    elif wc_pk in nodes_pk:\n",
    "        print('node_pk-{} is exist in the group-{}.'.format(wc_label, group.label))\n",
    "        return True, wc_pk\n",
    "              \n",
    "    else:\n",
    "        return False, None\n",
    "    \n",
    "    \n",
    "\n",
    "#### THIS FUNCTION IS SUCCESSFULLY TESTED ####\n",
    "\n",
    "## Trying to develop to delete the node from database\n",
    "## and at the same time from the remote folder\n",
    "\n",
    "## Add the following possible services\n",
    "# 1. Give the print option for how many decendant node will be \n",
    "#       and take the permission. Add option wether need to take \n",
    "#         permission or not.\n",
    "# 2. Also print how many calcjob node will be deleted under permission \n",
    "# 3. Print'remote directory' to check that all the data from the\n",
    "#     remote dir are deleted or not.\n",
    "## Technique\n",
    "# 1. Use the QuaryDB()\n",
    "# Use the cleandir\n",
    "\n",
    "\n",
    "\n",
    "#Section-1: delete node from the database as well as remote work directory\n",
    "### A function to delete the data of calculation output of calcjob list (pks) from the remote dir.\n",
    "\n",
    "# later add it with the del_node  function\n",
    "# This is successfully done\n",
    "\n",
    "# please note that it is tested for one calc list\n",
    "def delete_remote_workdir(pks, verbosity=0, dry_run= True):\n",
    "    from aiida.common import exceptions, NotExistent\n",
    "    from aiida.orm import load_node\n",
    "    from aiida.orm import computers\n",
    "    import sys\n",
    "    # TODO : add the verbosity as discused here \n",
    "    # https://aiida.readthedocs.io/projects/aiida-core/en/v1.5.0/_modules/aiida/manage/database/delete/nodes.html\n",
    "\n",
    "    \"\"\"\n",
    "    :param pks: calc node list\n",
    "    \n",
    "    \"\"\"\n",
    "    removed_path_list = [] # The part of the path will be deleted\n",
    "    remote_path_list = []  # The original path\n",
    "    updated_path_list = [] # After removing the part of the path\n",
    "    loadable_list = [] # To load the node and save it loadable_list\n",
    "    loaded_node_list = []\n",
    "    # To check the loadable calcjob list\n",
    "    for pk in pks:\n",
    "        try:\n",
    "            loaded_node = load_node(pk)\n",
    "        except exceptions.NotExistent:\n",
    "            print('This is calcjob node'.format(pk))\n",
    "            loaded_node = pk\n",
    "            loaded_node_list.append(loaded_node)\n",
    "            #             sys.exit()\n",
    "        else:\n",
    "            loaded_node_list.append(loaded_node)\n",
    "    # Computer data\n",
    "    \n",
    "   \n",
    "    for node in loaded_node_list:\n",
    "        load_pk = node\n",
    "        # computer data\n",
    "        computer = load_pk.computer\n",
    "        computer_name = computer.label\n",
    "        print(computer_name)\n",
    "        try:\n",
    "            remote_path = load_pk.get_remote_workdir()\n",
    "            \n",
    "        except NotExistent as ex:\n",
    "            print(f'Node (label,pk) ({node.label, node.pk}) does not have remote workdir. with exception : {ex}')\n",
    "        if isinstance(remote_path, str):\n",
    "            remote_path_list.append(remote_path)\n",
    "            delete_folder = remote_path.split('/')[-1]\n",
    "            removed_path_list.append(delete_folder)\n",
    "\n",
    "            new_remote_path = remote_path.replace(remote_path.split('/')[-1], '')\n",
    "            updated_path_list.append(new_remote_path)\n",
    "        \n",
    "        \n",
    "    if dry_run or verbosity == 3:\n",
    "\n",
    "        for i, paths in enumerate(zip(remote_path_list, updated_path_list)):\n",
    "            print('Before the delation the original path list : {}\\n'.format(paths[0]))\n",
    "            print('After deletion the modefied or updated path : {}'.format(paths[1]))\n",
    "    if verbosity == 3 or verbosity == 2:\n",
    "        val = input(\"Are you agree to clean the remote workdir (y/n) : \")\n",
    "    else:\n",
    "        val = 'y'\n",
    "    if str(val)=='y' or str(val)=='Y':\n",
    "        if not dry_run:\n",
    "            for remote_path in remote_path_list:\n",
    "                try:\n",
    "                    # Open the connection to the remote folder/dir via transport\n",
    "                    computer_transport = computer.get_transport()\n",
    "                    is_transport_open = computer_transport.is_open\n",
    "                    if not is_transport_open:\n",
    "                        computer_transport.open()\n",
    "                    computer_transport.rmtree(remote_path)\n",
    "                except IOError as ex:\n",
    "                    print('Uable to open the computer transport: ', ex)\n",
    "    else:\n",
    "        print('Nothing to clean from the remote workdir!')\n",
    "# section-5:delete node from the database as well as remote work directory\n",
    "## It returns all the calcjob from a WC node\n",
    "def find_calcJob(pk_or_node, debug=True):\n",
    "    \n",
    "    calcjob_node_list=[]\n",
    "    wc_node_list = []\n",
    "    try:\n",
    "        if isinstance( pk_or_node, int):\n",
    "            if debug:\n",
    "                print('This is pk')\n",
    "            node = load_node(pk_or_node)\n",
    "        else:\n",
    "            if debug:\n",
    "                print('This is node.')\n",
    "            node= pk_or_node\n",
    "    except:\n",
    "        print('{} is nither node ID nor aiida_node. '.format(pk_or_node))\n",
    "        \n",
    "    ## Use the get_calcjob_wc to get descendent calcjob list and  wc list\n",
    "    calc_list, wc_list = get_calcjob_wc(node)\n",
    "    calcjob_node_list += calc_list\n",
    "    \n",
    "    while len(wc_list)!=0:\n",
    "        new_wc_list = []\n",
    "\n",
    "        for i in wc_list[:]:\n",
    "            calc_list, wc_list = get_calcjob_wc(i)\n",
    "            new_wc_list += wc_list\n",
    "            calcjob_node_list += calc_list\n",
    "            \n",
    "        wc_list = new_wc_list\n",
    "\n",
    "    return calcjob_node_list\n",
    "\n",
    "## This function returns calcjob_list and wc_list from a wc or calcjob node   \n",
    "def get_calcjob_wc(node):\n",
    "    \"\"\"\n",
    "    :param: node\n",
    "    :return: workchain node list and calcjob node list\n",
    "    \"\"\" \n",
    "    from aiida.orm import CalcJobNode, WorkChainNode\n",
    "    wc = []\n",
    "    calc_job = []\n",
    "    \n",
    "    if node.node_type == 'process.workflow.workchain.WorkChainNode.':\n",
    "        \n",
    "    # here all outgoing worchain node\n",
    "        out_going_wc = node.get_outgoing(node_class=WorkChainNode).all()\n",
    "        wc = [i.node for i in out_going_wc[:]]\n",
    "        \n",
    "    # here all outgoing calcjob node\n",
    "        out_going_calc = node.get_outgoing(node_class=CalcJobNode).all()\n",
    "        calc_job = [i.node for i in out_going_calc[:]]\n",
    "                    \n",
    "    elif node.node_type == 'process.calculation.calcjob.CalcJobNode.':\n",
    "        calc_job.append(node)\n",
    "    \n",
    "    return calc_job, wc\n",
    "\n",
    "# This is the final del_node_function. Using this function for any specific wc node the node from the \n",
    "# Db as well as the calcjob data from the remote workdir can be deleted.\n",
    "def del_node(node_pks, dry_run=True, verbosity=3, debug=True, only_remote_dir=False,\n",
    "            only_database=False):\n",
    "    \"\"\"\n",
    "    1. This function will delete the node data from the database and also from the remote_dir\n",
    "    \n",
    "    :params node_pks: (list) list of workchain to delete from database as well as from remote workdir\n",
    "    :param verbosity: 0 prints nothing.  This is for workdir and wc\n",
    "                      1 prints just sums and total.   This is for workdir but not for wc\n",
    "                      2 prints indivisual nodes.  This is for workdir and wc\n",
    "    :param dry_run: Do not delete anything just show the status as in the verbosity given\n",
    "    \"\"\"\n",
    "    from aiida.orm import load_node\n",
    "    from aiida.manage.database.delete.nodes import delete_nodes\n",
    "    \n",
    "        \n",
    "    calcjobs_list = []\n",
    "    pks_given = []\n",
    "    for i in node_pks:    \n",
    "        try:\n",
    "            if isinstance( i, int):\n",
    "                if debug:\n",
    "                    print('This might be pk or uiid')\n",
    "                node = load_node(i)\n",
    "            else:\n",
    "                if debug:\n",
    "                    print('This might be a node.')\n",
    "                node= i\n",
    "        except:\n",
    "            print('{} is nither node ID nor aiida_node. '.format(i))\n",
    "        \n",
    "        pks_given.append(node.pk)\n",
    "        \n",
    "        calcjobs = find_calcJob(node, debug)\n",
    "        calcjobs_list += calcjobs\n",
    "        print('calcjob list : ', calcjobs_list,)\n",
    "    if only_remote_dir:\n",
    "        delete_remote_workdir(calcjobs_list, verbosity=verbosity, dry_run=dry_run)\n",
    "    if only_database:\n",
    "        delete_nodes(pks_given, verbosity=verbosity, dry_run=dry_run,force=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "certified-tender",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One very frequently used option\n",
    "metadata_option_1 = {'max_wallclock_seconds': 36000,'resources': \n",
    "               {'tot_num_mpiprocs': 48, 'num_machines': 1},\n",
    "              'custom_scheduler_commands': \n",
    "              '#SBATCH --account=jara0191\\n\\nulimit -s unlimited; export OMP_STACKSIZE=2g',\n",
    "              'withmpi': True\n",
    "                    }\n",
    "oscar_matadata = {'max_wallclock_seconds': 8*60*60,\n",
    "                  'resources':{'tot_num_mpiprocs': 12, \n",
    "                  'num_machines': 4},\n",
    "                  'custom_scheduler_commands': '#SBATCH -p oscar --nodes=1 --ntasks=12\\n\\nulimit -s unlimited; export OMP_STACKSIZE=2g',\n",
    "                  'withmpi':True\n",
    "                 }\n",
    "\n",
    "oscar_matadata = {'withmpi': True,\n",
    "         'resources': {'num_machines': 1, 'tot_num_mpiprocs': 12},\n",
    "         'queue_name': 'oscar',\n",
    "         'max_wallclock_seconds': 86400}\n",
    "\n",
    "voro_code = Code.get_from_string('voro@claix18_init')\n",
    "kkr_code = Code.get_from_string('kkr@claix18_init')\n",
    "kkrimp_code = Code.get_from_string('kkrflex@claix18_init')\n",
    "iffslurm_voro = Code.get_from_string('voro@iffslurm')\n",
    "iffslurm_kkr= Code.get_from_string('kkr@iffslurm')\n",
    "iffslurm_kkrimp= Code.get_from_string('kkrflex@iffslurm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "approximate-fiber",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that here for doped 3d and 4d atoms into the Bi2Te3 host saved in group-74\n",
    "\n",
    "group_74 = load_group(74)\n",
    "## first node of group-74 is for kkr_imp_wc process\n",
    "# is being used for the host_gf or kkr_flex files\n",
    "\n",
    "imp_wc_with_gf = load_node(16100)\n",
    "imp1_wc = group_74.nodes[1]\n",
    "imp2_wc = group_74.nodes[2]\n",
    "\n",
    "# gf_host_remote = imp_wc_with_gf.inputs.remote_data_gf\n",
    "# host_gf_wc = imp_wc_with_gf.get_outgoing(node_class=kkr_flex_wc).all()[0].node\n",
    "\n",
    "\n",
    "impurity1_output_node = imp1_wc.outputs.workflow_info\n",
    "impurity2_output_node = imp2_wc.outputs.workflow_info\n",
    "offset_imp2 = {'index':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "raising-thunder",
   "metadata": {},
   "outputs": [],
   "source": [
    "scf_node = imp1_wc.get_outgoing(node_class=kkr_imp_sub_wc).all()[0].node\n",
    "## kkr_flex_wc\n",
    "host_gf = imp1_wc.inputs.remote_data_gf.get_incoming(node_class=kkr_flex_wc).all()[0].node\n",
    "kkr_calc = host_gf.get_outgoing(node_class=KkrCalculation).all()[0].node\n",
    "kkr_flex_kkr_param = kkr_calc.inputs.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "mental-grace",
   "metadata": {},
   "outputs": [],
   "source": [
    "kkr_flex_kkr_param = kkr_calc.inputs.parameters.get_dict()\n",
    "kkr_flex_kkr_param['RCLUSTZ'] *= 1\n",
    "kkr_flex_kkr_param['NSHELD'] = 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "muslim-object",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version of workflow: 0.3.1\n"
     ]
    }
   ],
   "source": [
    "settings = combine_imps_wc.get_wf_defaults()\n",
    "builder = combine_imps_wc.get_builder()\n",
    "settings['jij_run']= True\n",
    "settings['retrieve_kkrflex'] = False\n",
    "settings['mag_init']= False\n",
    "settings['threshold_switch_high_accuracy'] = 1e-2\n",
    "settings['convergence_criterion'] = 1e-7 #convergence_criterion\n",
    "settings['threshold_aggressive_mixing']= 0.05\n",
    "settings['strmix']= 0.01\n",
    "settings['aggrmix'] = 0.05\n",
    "settings['broyden-number'] = 20\n",
    "settings['nsteps'] = 100\n",
    "settings['kkr_runmax'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "chief-looking",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = scf_node.inputs.options.get_dict()\n",
    "options['max_wallclock_seconds'] = 36000*1\n",
    "# options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "stone-coating",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder.impurity1_output_node = impurity1_output_node\n",
    "builder.impurity2_output_node = impurity2_output_node\n",
    "builder.offset_imp2 = Dict(dict=offset_imp2)\n",
    "\n",
    "# scf namespace setup\n",
    "builder.scf.kkrimp = scf_node.inputs.kkrimp\n",
    "builder.scf.options = Dict(dict=options)\n",
    "builder.scf.wf_parameters = scf_node.inputs.wf_parameters\n",
    "\n",
    "# host_gf namespace setup\n",
    "builder.host_gf.kkr = host_gf.inputs.kkr\n",
    "builder.host_gf.options = host_gf.inputs.options\n",
    "builder.host_gf.params_kkr_overwrite = Dict(dict=kkr_flex_kkr_param) #host_gf.inputs.wf_parameters\n",
    "\n",
    "builder.wf_parameters_overwrite = Dict(dict=settings)\n",
    "builder.metadata.label = str(imp1_wc.label+ ':' +imp2_wc.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "honest-danish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To submit the builder\n",
    "# combined_imp_submission = submit(builder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "assured-sector",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version of workflow: 0.3.1\n",
      "label :  Sc:Ti:Eu:Bi2Te3_il_3_3_3_Off_1_2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<WorkChainNode: uuid: 9d0d1172-9291-4fbc-a5d5-5743f4a3e0dd (pk: 102260) (aiida_kkr.workflows._combine_imps.combine_imps_wc)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that here for doped 3d atoms into the Bi2Te3 host\n",
    "\n",
    "group_102= load_group(102)# label: 3d_4d_dope_Bi2Te3_il_3:il_3_offset_1\n",
    "nodes_102= list(group_102.nodes)\n",
    "\n",
    "group_113= load_group(113)  #label: SomeOtherImpDopants_Bi2Te3_ilayer_3\n",
    "nodes_113= list(group_113.nodes)\n",
    "\n",
    "imp1_wc = nodes_102[0]\n",
    "imp2_wc = nodes_113[0]\n",
    "\n",
    "scf = imp1_wc\n",
    "\n",
    "host_kkr = Code().get_from_string('kkr@claix18_init')\n",
    "\n",
    "scf_kkrimp = Code().get_from_string('kkrflex@claix18_init')\n",
    "options = Dict(dict=metadata_option_1)\n",
    "\n",
    "imp1_output_node = imp1_wc.outputs.workflow_info\n",
    "imp2_output_node = imp2_wc.outputs.workflow_info\n",
    "offset_imp2 = {'index':2}\n",
    "\n",
    "## For kkr_writeout_step: parameters for kkr calculation\n",
    "params_kkr_overwrite = {'BZDIVIDE': [80,80,80],\n",
    "                        'strmix': 0.001,\n",
    "                        'NSHELD': 2500, \n",
    "                        'KPOIBZ': 1000000 # To increase the KPOIBZ(NxNyNz= 100*100*100)\n",
    "                       }\n",
    "params_kkr_overwrite= Dict(dict=params_kkr_overwrite)\n",
    "\n",
    "settings = combine_imps_wc.get_wf_defaults()\n",
    "settings['jij_run'] = True\n",
    "settings['dos_run'] = False\n",
    "settings['lmdos'] = False\n",
    "settings['strmix'] = 0.01\n",
    "settings['aggrmix'] = 0.05\n",
    "settings['threshold_aggressive_mixing'] = 0.01\n",
    "settings['convergence_criterion'] = 1e-8\n",
    "settings['threshold_switch_high_accuracy'] = 1e-2\n",
    "settings['retrieve_kkrflex'] = True\n",
    "settings['mag_init'] = False\n",
    "settings['mixreduce'] = 0.5\n",
    "settings['nsteps'] = 100\n",
    "settings['kkr_runmax'] = 10\n",
    "settings['broyden-number'] = 20\n",
    "\n",
    "offset_index= offset_imp2['index']\n",
    "\n",
    "label = node_label_creation(imp1_wc, imp2_wc, offset_index)\n",
    "print('label : ',label)\n",
    "submit_combine_imp(imp1_wc_node=imp1_wc, imp2_wc_node=imp2_wc, kkr_code=host_kkr, kkr_imp_code=scf_kkrimp, offset_imp2=offset_imp2, \n",
    "                    options = options, scf_wf_parameters=None, params_kkr_overwrite=params_kkr_overwrite,settings=settings, \n",
    "                    gf_host_remote=None, dry_run=False, label=label\n",
    "                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-gnome",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here trying to work with host Au and other impurity\n",
    "## THe calc is taken from the wasmer\n",
    "imp_Au_group = load_group(85)\n",
    "wc_list = list(imp_Au_group.nodes)\n",
    "\n",
    "\n",
    "imp_wc_1 = wc_list[0]\n",
    "# print('first imp wc : ', imp_wc_1.label)\n",
    "imp_wc_2 = wc_list[1]\n",
    "# print('econd imp wc : ', imp_wc_2.label)\n",
    "# setting the scf and host namespace\n",
    "scf = imp_wc_1\n",
    "\n",
    "host_wf_parameters = Dict(dict={'NSHELD' : 2500,\n",
    "                               })\n",
    "\n",
    "host_kkr = Code().get_from_string('kkr@claix18_init')\n",
    "scf_kkrimp = Code().get_from_string('kkrflex@claix18_init')\n",
    "options = Dict(dict=metadata_option_1)\n",
    "\n",
    "imp1_output_node = imp_wc_1.outputs.workflow_info\n",
    "imp2_output_node = imp_wc_2.outputs.workflow_info\n",
    "offset_imp2 = {'index':1}\n",
    "\n",
    "settings = combine_imps_wc.get_wf_defaults()\n",
    "settings['jij_run'] = False\n",
    "settings['lmdos'] = False\n",
    "settings['strmix'] = 0.001\n",
    "settings['aggrmix'] = 0.06\n",
    "settings['threshold_aggressive_mixing'] = 0.06\n",
    "settings['convergence_criterion'] = 1e-7\n",
    "settings['threshold_switch_high_accuracy'] = 1e-2\n",
    "settings['retrieve_kkrflex'] = False\n",
    "settings['mag_init'] = False\n",
    "settings['mixreduce'] = 0.5\n",
    "settings['nsteps'] = 100\n",
    "settings['kkr_runmax'] = 10\n",
    "\n",
    "combine_imp_builder = combine_imps_wc.get_builder()\n",
    "combine_imp_builder.impurity1_output_node = imp1_output_node\n",
    "combine_imp_builder.impurity2_output_node = imp2_output_node\n",
    "combine_imp_builder.offset_imp2 = Dict(dict=offset_imp2)\n",
    "\n",
    "# scf namespace setup\n",
    "combine_imp_builder.scf.kkrimp = iffslurm_kkrimp\n",
    "combine_imp_builder.scf.options = Dict(dict=oscar_matadata)\n",
    "combine_imp_builder.scf.wf_parameters = scf.inputs.wf_parameters\n",
    "# combine_imp_builder.gf_host_remote\n",
    "# host_gf namespace setup\n",
    "combine_imp_builder.host_gf.kkr = iffslurm_kkr\n",
    "combine_imp_builder.host_gf.options = Dict(dict=oscar_matadata)\n",
    "combine_imp_builder.host_gf.params_kkr_overwrite = host_wf_parameters #host_gf.inputs.wf_parameters\n",
    "\n",
    "combine_imp_builder.wf_parameters_overwrite = Dict(dict=settings)\n",
    "combine_imp_builder.metadata.label = str(imp1_wc.label+ ':' +imp2_wc.label)\n",
    "#submission = submit(combine_imp_builder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-canvas",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_74 = load_group(74)\n",
    "group_91 = load_group(91)\n",
    "group_74 = list(group_74.nodes)\n",
    "group_91 = list(group_91.nodes)\n",
    "# 3vs3 indx1\n",
    "# 3vs4 indx1\n",
    "###Code list\n",
    "kkr_code = Code.get_from_string('kkr@claix18_init')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behavioral-wesley",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepartion to submit il-3 Vs il-4 for the 3d_&_4d block\n",
    "\n",
    "\n",
    "# extract from parent wc\n",
    "## node 24418 is succefull wc for li-3 il-3 from node 16215 and 16237\n",
    "## node 24493 is succefull wc for li-4 il-4 from node 18853 and 18853\n",
    "## node 24510 is succefull wc for li-4 il-4 from node 18853 and 16215\n",
    "\n",
    "#### This part from group 74 and 91 ----------------------------\n",
    "## To run the il 3 vs 4 please run from this pk: 67687 but before test the label\n",
    "parent_wc = load_node(67687)\n",
    "## This part for the failed combined calculations by increasing the BZDIVIDE-------------\n",
    "params_kkr_overwrite = {'BZDIVIDE': [80,80,80],\n",
    "                        'strmix': 0.02,\n",
    "                        'NSHELD': 2500, \n",
    "                        'KPOIBZ': 1000000 # To increase the KPOIBZ(NxNyNz= 100*100*100)\n",
    "                       }\n",
    "## ------------------------------------------\n",
    "\n",
    "kkr_imp_code = parent_wc.inputs.scf__kkrimp\n",
    "options = Dict(dict=metadata_option_1)\n",
    "scf_wf_parameters = parent_wc.inputs.scf__wf_parameters\n",
    "\n",
    "settings = parent_wc.inputs.wf_parameters_overwrite\n",
    "## If need use the customised settings\n",
    "settings_customised = {}\n",
    "settings_customised['jij_run'] = True\n",
    "settings_customised['lmdos'] = False\n",
    "settings_customised['strmix'] = 0.02\n",
    "settings_customised['aggrmix'] = 0.008\n",
    "settings_customised['threshold_aggressive_mixing'] = 0.01  ## it takes the first scf limit\n",
    "settings_customised['convergence_criterion'] = 1e-8\n",
    "settings_customised['threshold_switch_high_accuracy'] = 1e-2\n",
    "settings_customised['retrieve_kkrflex'] = True\n",
    "settings_customised['mag_init'] = False\n",
    "settings_customised['mixreduce'] = 0.5\n",
    "settings_customised['nsteps'] = 120\n",
    "settings_customised['kkr_runmax'] = 10\n",
    "\n",
    "gf_host_remote = parent_wc.outputs.remote_data_gf\n",
    "\n",
    "offset_imp2 = parent_wc.inputs.offset_imp2\n",
    "\n",
    "### In case to change the offset_number\n",
    "offset_imp2= offset_imp2.get_dict()\n",
    "offset_imp2['index']= 2\n",
    "offset_imp2= Dict(dict=offset_imp2)\n",
    "print(offset_imp2.get_dict())\n",
    "### end here to change the offset_imp2\n",
    "\n",
    "si_imp_list1 = group_74\n",
    "si_imp_list2 = group_91\n",
    "\n",
    "succ_group_label = '3d_4d_dope_Bi2Te3_il_3:il_4_offset_2'\n",
    "succ_group_descr = 'only from group ID 74 and 91'\n",
    "\n",
    "#submit_double_imp_wc_bunch( si_imp_list1= si_imp_list1, \n",
    "                             si_imp_list2= si_imp_list2, \n",
    "                             kkr_code=None, #kkr_code, \n",
    "                             kkr_imp_code= kkr_imp_code, \n",
    "                             builder_options= options,  \n",
    "                             succ_group_label= succ_group_label,\n",
    "                             succ_group_descr= succ_group_descr,\n",
    "                             offset_imp2= offset_imp2,\n",
    "                             max_submission= 50,\n",
    "                             setting = Dict(dict=settings_customised), \n",
    "                             gf_host_remote= gf_host_remote,\n",
    "                             scf_wf_parameters= scf_wf_parameters, \n",
    "                             params_kkr_overwrite= Dict(dict=params_kkr_overwrite),\n",
    "                             dry_run= False,\n",
    "                             max_fail_wc= 5,\n",
    "                             debug= False\n",
    "                               )\n",
    "## \n",
    "#### This part from group 74 and 91 ----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-graduation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepartion to submit il-3 Vs il-3 for the 3d_&_4d block\n",
    "\n",
    "\n",
    "# extract from parent wc\n",
    "## node 24418 is succefull wc for li-3 il-3 from node 16215 and 16237\n",
    "## node 24493 is succefull wc for li-4 il-4 from node 18853 and 18853\n",
    "## node 24510 is succefull wc for li-4 il-4 from node 18853 and 16215\n",
    "\n",
    "##This part for not successful wc to retry them------\n",
    "params_kkr_overwrite = {'BZDIVIDE': [80,80,80],\n",
    "                        'strmix': 0.01,\n",
    "                        'NSHELD': 2500,\n",
    "                        'KPOIBZ': 1000000 ## To increase the KPOIBZ: NxNyNz=100 100 100\n",
    "                       }\n",
    "##---------------------------------------------------\n",
    "parent_wc = load_node(67670)\n",
    "\n",
    "# To run the il_3_3 for offset index 2 try with pk: 67670 is it is successful and also before starting please checck the label\n",
    "\n",
    "#### This part from group 74 and 91 ----------------------------\n",
    "\n",
    "settings_customised = {}\n",
    "# settings_customised['RCLUSTZ'] = 2.3\n",
    "settings_customised['jij_run'] = True\n",
    "settings_customised['strmix'] = 0.001\n",
    "settings_customised['aggrmix'] = 0.005 ## Maybe it takes the first scf limit\n",
    "# settings_customised['threshold_aggressive_mixing'] = 0.02\n",
    "settings_customised['convergence_criterion'] = 1e-8\n",
    "settings_customised['threshold_switch_high_accuracy'] = 1e-2\n",
    "settings_customised['retrieve_kkrflex'] = False\n",
    "settings_customised['mag_init'] = False\n",
    "settings_customised['mixreduce'] = 0.2\n",
    "settings_customised['nsteps'] = 100\n",
    "settings_customised['kkr_runmax'] = 10\n",
    "\n",
    "\n",
    "\n",
    "kkr_imp_code = parent_wc.inputs.scf__kkrimp\n",
    "options = parent_wc.inputs.scf__options\n",
    "scf_wf_parameters = parent_wc.inputs.scf__wf_parameters\n",
    "\n",
    "settings = parent_wc.inputs.wf_parameters_overwrite\n",
    "\n",
    "gf_host_remote = parent_wc.outputs.remote_data_gf\n",
    "# params_kkr_overwrite = parent_wc.\n",
    "\n",
    "offset_imp2 = parent_wc.inputs.offset_imp2\n",
    "### In case to change the offset_number\n",
    "offset_imp2 = offset_imp2.get_dict()\n",
    "offset_imp2['index']= 2\n",
    "offset_imp2 = Dict(dict=offset_imp2)\n",
    "print(offset_imp2.get_dict())\n",
    "### end here to change the offset_imp2\n",
    "\n",
    "si_imp_list1 = group_74\n",
    "si_imp_list2 = group_74\n",
    "\n",
    "succ_group_label = '3d_4d_dope_Bi2Te3_il_3:il_3_offset_2'\n",
    "succ_group_descr = 'only from group ID 74 and 74'\n",
    "\n",
    "#submit_double_imp_wc_bunch( si_imp_list1= si_imp_list1, \n",
    "                            si_imp_list2= si_imp_list2, \n",
    "                            kkr_code= None, #kkr_code, \n",
    "                            kkr_imp_code= kkr_imp_code, \n",
    "                            builder_options= options,  \n",
    "                            succ_group_label= succ_group_label,\n",
    "                            succ_group_descr= succ_group_descr,\n",
    "                            offset_imp2= offset_imp2,\n",
    "                            max_submission= 50,\n",
    "                            setting= settings_customised, \n",
    "                            gf_host_remote= gf_host_remote,\n",
    "                            scf_wf_parameters= scf_wf_parameters, \n",
    "#                            params_kkr_overwrite= Dict(dict=params_kkr_overwrite),\n",
    "                            dry_run= True,\n",
    "                            max_fail_wc= 5,\n",
    "                            debug= False\n",
    "                        )\n",
    "#first combine calc=67634(F, Del), 67653(checck, Del)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweet-exhibit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete all the node in 106\n",
    "group_106= load_group(122)# 73\n",
    "nodes= list(group_106.nodes)\n",
    "\n",
    "# DElete only remote data:102222\n",
    "\n",
    "#del_node(node_pks=nodes, dry_run=False, verbosity=3, debug=True, only_remote_dir=True, only_database=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-explosion",
   "metadata": {},
   "outputs": [],
   "source": [
    "transport.Transport.is_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-ceremony",
   "metadata": {},
   "outputs": [],
   "source": [
    "qb1 = QueryBuilder()\n",
    "Qb_node_list = qb1.append(WorkChainNode,\n",
    "          #filters={\n",
    "#               'and':[\n",
    "#                   {'attributes.process_label':'kkr_imp_wc'},\n",
    "#                   {'attributes.exit_status':{'!in':[0]}}\n",
    "                  \n",
    "#               ],\n",
    "             # 'id':{'>':57959},\n",
    " #             'attributes.process_label':'kkr_imp_wc',\n",
    "#\n",
    "              \n",
    " #         }\n",
    "         ).all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AiiDA",
   "language": "python",
   "name": "aiida"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
