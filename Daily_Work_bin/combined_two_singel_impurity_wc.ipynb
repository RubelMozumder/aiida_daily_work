{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "white-pharmaceutical",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the aiida profile\n",
    "from aiida import load_profile\n",
    "aiida_profile = load_profile()\n",
    "aiida_profile.name\n",
    "\n",
    "## Loading the some require packages and module\n",
    "from aiida_kkr.workflows import (combine_imps_wc, kkr_flex_wc,\n",
    "                                 kkr_imp_wc, kkr_imp_sub_wc\n",
    "                                )\n",
    "from aiida_kkr.calculations import KkrCalculation, KkrimpCalculation\n",
    "from aiida.orm import (Group, load_group, load_node, Dict, Code\n",
    "                      )\n",
    "from aiida_kkr.tools import kkrparams\n",
    "from aiida.engine import submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inclusive-jordan",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "british-update",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An attemp to run a bunch of wc of the double impurity from two groups of single impurity wc or \n",
    "# two list of the single impurity wc\n",
    "\n",
    "def submit_double_imp_wc_bunch( si_imp_list1, si_imp_list2, kkr_code, kkr_imp_code, builder_options,  \n",
    "                                succ_group_label, succ_group_descr, offset_imp2={'index':1}, \n",
    "                                max_submission=20, setting=None, gf_host_remote=None,\n",
    "                                scf_wf_parameters=None, params_kkr_overwrite=None, dry_run= True,\n",
    "                                max_fail_wc = 10, debug= False\n",
    "                              ):\n",
    "    import time as t\n",
    "    \"\"\"\n",
    "    params: \n",
    "        : Two lists : (si_imp_list1, si_imp_list2) : Two single imp wc list for combination\n",
    "        : kkr_imp_code : kkr_imp code for kkr_imp_sub_wc\n",
    "        : kkr_code : kkr_code for kkr_flex_wc\n",
    "        : Builder_options Dict : computer settings.\n",
    "        : succ_group_label str : Create a group by this name if it does not exist in the DB.\n",
    "        : succ_group_descr str : Set the created group description either it exists or not, it not a new group\n",
    "                            will be created.\n",
    "        : offset_imp2 : {'index':1}; for 0,1,.. same unit cell or nearest unit cell and so on.\n",
    "        : max_submissioin : This is the max number of wc for submission runing at the same time.\n",
    "        : Settings Dict ot dict : Parameter settings.\n",
    "        : gf_host_remote : RemoteData : Remote kkr flex files\n",
    "        : scf_wf_parameters : wf_parameters for the scf.wf_parameters\n",
    "        : params_kkr_overwrite : wf_parameters for host.wf_parameters\n",
    "        : dry_run : Just for check the code\n",
    "        : max_fail_wc = stop submission if failed calculation is equal or more than max_fail_wc\n",
    "        : debug : Also for check the coding\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create group or load existing group\n",
    "    succ_group = group_not_exist_create(group_label=succ_group_label, group_descr=succ_group_descr)\n",
    "    fail_group = group_not_exist_create(group_label=succ_group_label+'_fail', group_descr=succ_group_descr+'_fail')\n",
    "    \n",
    "    \n",
    "#     test_data and prepare for further\n",
    "    if not isinstance(si_imp_list1, list):\n",
    "        print('The given impurity wc list 1 is not the list type.')\n",
    "        return print('Please provide single_imp1_list.')\n",
    "    if not isinstance(si_imp_list2, list):\n",
    "        print('The given impurity wc list 2 is not the list type.')\n",
    "        return print('Please provide single_imp2_list.')\n",
    "    \n",
    "    all_combination_dict = {}\n",
    "    all_success_dict = {}\n",
    "    all_submission_dict = {}\n",
    "    all_resedue_dict = {}\n",
    "    \n",
    "    tot_wc_num = 0\n",
    "    all_submission_num = 0\n",
    "    all_success_num = 0\n",
    "    all_failed_num = 0\n",
    "    \n",
    "    # Here to create all the possible dict\n",
    "    for i in si_imp_list1[:]:\n",
    "        for j in si_imp_list2[:]:\n",
    "            node_truple = (i, j)\n",
    "            pk_truple = (i.pk, j.pk)\n",
    "            imp1_info = i.inputs.impurity_info.get_dict()\n",
    "            imp2_info = j.inputs.impurity_info.get_dict()\n",
    "            ilayer1 = str(imp1_info['ilayer_center'])\n",
    "            ilayer2 = str(imp2_info['ilayer_center'])\n",
    "            label= i.label.split(':')[0] + ':'+ j.label+'_il_'+ilayer1+'_il_'+ilayer2        \n",
    "            all_combination_dict[tot_wc_num] = {'node_truple' : node_truple,\n",
    "                                                'pk_truple' : pk_truple,\n",
    "                                                'label' : label,\n",
    "                                                'submission': None}\n",
    "            tot_wc_num +=1\n",
    "    \n",
    "    all_resedue_dict = all_combination_dict.copy()\n",
    "    \n",
    "    # To start the submission process, save the success nodes and failed nodes into the corresponding\n",
    "    # group\n",
    "    N=0\n",
    "    for key, val in all_combination_dict.items():\n",
    "\n",
    "        imp1_wc_node = all_combination_dict[key]['node_truple'][0]\n",
    "        imp2_wc_node = all_combination_dict[key]['node_truple'][1]\n",
    "        label = all_combination_dict[key]['label']\n",
    "       \n",
    "        wc_pre_exist = check_wc_exist_in_group(succ_group, wc_label=label)\n",
    "        wc_pre_fail = check_wc_exist_in_group(fail_group, wc_label=label)\n",
    "        \n",
    "        if wc_pre_exist or wc_pre_fail:\n",
    "#             print('Already one wc named as {} is exist in this group'.format(label))\n",
    "            N += 1\n",
    "            # To reduce the tot_wc_num as they are either failed or sucessfully finished earlier\n",
    "            tot_wc_num -= 1\n",
    "            continue\n",
    "        else:\n",
    "            print('val : ', val)\n",
    "        \n",
    "        if all_failed_num == max_fail_wc:\n",
    "            print('This is for loop break, as all_failed_num == max_fail_wc')\n",
    "            break\n",
    "        \n",
    "        if debug: \n",
    "            print('imp1_pk for submission', imp1_wc_node.pk)\n",
    "            print('imp2_pk for submission', imp2_wc_node.pk)\n",
    "\n",
    "\n",
    "        if not dry_run: \n",
    "            submission = submit_combine_imp(imp1_wc_node, imp2_wc_node, kkr_code, kkr_imp_code, offset_imp2,\n",
    "                                            options, settings, dry_run, label, gf_host_remote,\n",
    "                                            scf_wf_parameters, params_kkr_overwrite )\n",
    "            print(f\"THe submitted combine Imps is : {submission.pk}\")\n",
    "            all_submission_dict[key] = all_combination_dict[key].copy()\n",
    "            all_submission_dict[key]['submission'] = submission\n",
    "            all_submission_num += 1\n",
    "        while((all_submission_num - all_success_num - all_failed_num >= max_submission) or \n",
    "                (tot_wc_num - all_submission_num == 0) or (all_failed_num == max_fail_wc)):\n",
    "                t.sleep(60*3)\n",
    "                \n",
    "#----------------------------------------------------------------------------------\n",
    "\n",
    "                pop_list = []\n",
    "                for submit_key in all_submission_dict.keys():\n",
    "                    submission = all_submission_dict[submit_key]['submission']\n",
    "                    if submission.is_finished == True:\n",
    "                        if submission.exit_status == 0 : \n",
    "                            all_success_num +=1\n",
    "                            print(f'all_success_num : {all_success_num}, and last pk : {submission.pk}') \n",
    "                            all_success_dict[submit_key] = all_combination_dict[submit_key].copy()\n",
    "                            all_resedue_dict.pop(submit_key)\n",
    "\n",
    "                            succ_group.add_nodes(submission)\n",
    "                            pop_list.append(submit_key)\n",
    "                            del_node(node_pks=[submission.pk], dry_run=False, verbosity=0, debug=False, only_remote_dir=True)\n",
    "                        else:\n",
    "                            all_failed_num += 1\n",
    "                            print(f'all_failed_num : {all_failed_num}, last pk : {submission.pk}')\n",
    "                            fail_group.add_nodes(submission)\n",
    "                            pop_list.append(submit_key)\n",
    "                            \n",
    "                    elif submission.is_excepted:\n",
    "                        all_failed_num += 1\n",
    "                        print(f'all_excepted_num : {all_failed_num}, last pk : {submission.pk}')\n",
    "                        fail_group.add_nodes(submission)\n",
    "                        pop_list.append(submit_key)\n",
    "                        \n",
    "#---------------------------------------------------------------------------------\n",
    "                garbage = [all_submission_dict.pop(pop_key) for pop_key in pop_list[:]]\n",
    "                    \n",
    "                if len(all_submission_dict) == 0 :\n",
    "                    print('This is while loop break, as all_submission_dict.')\n",
    "                    break\n",
    "                if all_failed_num == max_fail_wc :\n",
    "                    if (all_submission_num - all_success_num - all_failed_num) == 0 :\n",
    "                        print('This while loop break, as (all_submission_num - all_success_num - all_failed_num) == 0')\n",
    "                        break\n",
    "                \n",
    "    print('total failed or sucsess wc : ', N)\n",
    "#     return all_success_dict, all_resedue_dict\n",
    "            \n",
    "\n",
    "def submit_combine_imp(imp1_wc_node, imp2_wc_node, kkr_code, kkr_imp_code, offset_imp2, options, \n",
    "                       settings, dry_run, label, gf_host_remote=None,\n",
    "                       scf_wf_parameters = None, params_kkr_overwrite=None):\n",
    "    \n",
    "    from aiida_kkr.workflows import combine_imps_wc, kkr_imp_sub_wc, kkr_flex_wc\n",
    "    from aiida.orm import Dict\n",
    "    imp1_output = imp1_wc_node.outputs.workflow_info\n",
    "    imp2_output = imp2_wc_node.outputs.workflow_info\n",
    "    if scf_wf_parameters==None:\n",
    "        sub_wc1_node = imp1_wc_node.get_outgoing(node_class=kkr_imp_sub_wc).first().node\n",
    "        scf_wf_parameters = sub_wc1_node.inputs.wf_parameters\n",
    "    \n",
    "    if gf_host_remote==None:\n",
    "        if params_kkr_overwrite==None:\n",
    "            ##TODO: In case the gf_remote is from the parent calc: Another possible way to do it as, from inputs remote_gf->kkr_flex_wc \n",
    "            sub_gf_write_out = imp1_wc_node.get_outgoing(node_class=kkr_flex_wc).first().node\n",
    "                \n",
    "            params_kkr_overwrite = sub_gf_write_out.inputs.params_kkr_overwrite\n",
    "    if settings==None:\n",
    "        settings = imp1_wc_node.inputs.wf_parameters_overwrite\n",
    "    if label==None:\n",
    "        label = 'pk' + str(imp1_wc_node.pk)+':'+ str(imp1_wc_node.pk)\n",
    "    \n",
    "    builder = combine_imps_wc.get_builder()\n",
    "    builder.impurity1_output_node = imp1_output\n",
    "    builder.impurity2_output_node = imp2_output\n",
    "    if isinstance(offset_imp2, dict):\n",
    "        builder.offset_imp2 = Dict(dict=offset_imp2)\n",
    "    else:\n",
    "        builder.offset_imp2 = offset_imp2\n",
    "    \n",
    "    builder.scf.kkrimp = kkr_imp_code\n",
    "    builder.scf.options = options\n",
    "    builder.scf.wf_parameters = scf_wf_parameters\n",
    "    \n",
    "    if gf_host_remote==None:\n",
    "        builder.host_gf.kkr = kkr_code\n",
    "        builder.host_gf.options = options\n",
    "        builder.host_gf.params_kkr_overwrite = params_kkr_overwrite #host_gf.inputs.wf_parameters\n",
    "    else:\n",
    "        builder.gf_host_remote = gf_host_remote\n",
    "    if settings!=None:\n",
    "        if isinstance(settings, dict):\n",
    "            builder.wf_parameters_overwrite = Dict(dict=settings)\n",
    "        else:\n",
    "            builder.wf_parameters_overwrite = settings\n",
    "            \n",
    "    builder.metadata.label = label\n",
    "    if not dry_run:\n",
    "        submission = submit(builder)\n",
    "        return submission\n",
    "    else:\n",
    "        msg = ' This is dry_run. '\n",
    "        return msg\n",
    "    \n",
    "def group_not_exist_create(group_label, group_descr=None):\n",
    "    from aiida.orm import load_group, Group\n",
    "    \"\"\"\n",
    "        Check the group exist either must create\n",
    "    \"\"\"\n",
    "    if group_descr==None:\n",
    "        group_descr='No Description is added'\n",
    "    \n",
    "    try:\n",
    "         group = load_group(group_label)\n",
    "    except:\n",
    "        print('Group named {} is not exist but is being created .'.format(group_label))\n",
    "        group = Group(label=group_label, description=group_descr)\n",
    "        group.store()\n",
    "        print('Newly created group pk {}'.format(group.pk))\n",
    "    return group\n",
    "\n",
    "\n",
    "def check_wc_exist_in_group(group, wc_label=None, wc_pk=None):\n",
    "    nodes_list = list(group.nodes)\n",
    "    nodes_label = [i.label for i in nodes_list[:]]\n",
    "    nodes_pk = [i.pk for i in nodes_list[:]]\n",
    "    \n",
    "    if wc_label in nodes_label:\n",
    "        print('node_label-{} is exist in the group-{}.'.format(wc_label, group.label))\n",
    "        return True\n",
    "              \n",
    "    elif wc_pk in nodes_pk:\n",
    "        print('node_pk-{} is exist in the group-{}.'.format(wc_label, group.label))\n",
    "        return True\n",
    "              \n",
    "    else:\n",
    "        return False\n",
    "\n",
    "#### THIS FUNCTION IS SUCCESSFULLY TESTED ####\n",
    "\n",
    "## Trying to develop to delete the node from database\n",
    "## and at the same time from the remote folder\n",
    "\n",
    "## Add the following possible services\n",
    "# 1. Give the print option for how many decendant node will be \n",
    "#       and take the permission. Add option wether need to take \n",
    "#         permission or not.\n",
    "# 2. Also print how many calcjob node will be deleted under permission \n",
    "# 3. Print'remote directory' to check that all the data from the\n",
    "#     remote dir are deleted or not.\n",
    "## Technique\n",
    "# 1. Use the QuaryDB()\n",
    "# Use the cleandir\n",
    "\n",
    "\n",
    "\n",
    "#Section-1: delete node from the database as well as remote work directory\n",
    "### A function to delete the data of calculation output of calcjob list (pks) from the remote dir.\n",
    "\n",
    "# later add it with the del_node  function\n",
    "# This is successfully done\n",
    "\n",
    "# please note that it is tested for one calc list\n",
    "def delete_remote_workdir(pks, verbosity=0, dry_run= True):\n",
    "    from aiida.common import exceptions\n",
    "    from aiida.orm import load_node\n",
    "    from aiida.orm import computers\n",
    "    import sys\n",
    "    # TODO : add the verbosity as discused here \n",
    "    # https://aiida.readthedocs.io/projects/aiida-core/en/v1.5.0/_modules/aiida/manage/database/delete/nodes.html\n",
    "\n",
    "    \"\"\"\n",
    "    :param pks: calc node list\n",
    "    \n",
    "    \"\"\"\n",
    "    removed_path_list = [] # The part of the path will be deleted\n",
    "    remote_path_list = []  # The original path\n",
    "    updated_path_list = [] # After removing the part of the path\n",
    "    loadable_list = [] # To load the node and save it loadable_list\n",
    "    loaded_node_list = []\n",
    "    # To check the loadable calcjob list\n",
    "    for pk in pks:\n",
    "        try:\n",
    "            loaded_node = load_node(pk)\n",
    "        except exceptions.NotExistent:\n",
    "            print('This is calcjob node'.format(pk))\n",
    "            loaded_node = pk\n",
    "            loaded_node_list.append(loaded_node)\n",
    "            #             sys.exit()\n",
    "        else:\n",
    "            loaded_node_list.append(loaded_node)\n",
    "    # Computer data\n",
    "    \n",
    "   \n",
    "    for node in loaded_node_list:\n",
    "        load_pk = node\n",
    "        # computer data\n",
    "        computer = load_pk.computer\n",
    "        computer_name = computer.label\n",
    "        print(computer_name)\n",
    "        \n",
    "        remote_path = load_pk.get_remote_workdir()\n",
    "        remote_path_list.append(remote_path)\n",
    "        \n",
    "        delete_folder = remote_path.split('/')[-1]\n",
    "        removed_path_list.append(delete_folder)\n",
    "\n",
    "        new_remote_path = remote_path.replace(remote_path.split('/')[-1], '')\n",
    "        updated_path_list.append(new_remote_path)\n",
    "        \n",
    "        \n",
    "        \n",
    "    if dry_run or verbosity == 3:\n",
    "\n",
    "        for i, paths in enumerate(zip(remote_path_list, updated_path_list)):\n",
    "            print('Before the delation the original path list : {}\\n'.format(paths[0]))\n",
    "            print('After deletion the modefied or updated path : {}'.format(paths[1]))\n",
    "    if verbosity == 3 or verbosity == 2:\n",
    "        val = input(\"Are you agree to clean the remote workdir (y/n) : \")\n",
    "    else:\n",
    "        val = 'y'\n",
    "    if str(val)=='y' or str(val)=='Y':\n",
    "        if not dry_run:\n",
    "            for remote_path in remote_path_list:\n",
    "            \n",
    "                    # Open the connection to the remote folder/dir via transport\n",
    "                    computer_transport = computer.get_transport()\n",
    "                    is_transport_open = computer_transport.is_open\n",
    "                    if not is_transport_open:\n",
    "                        computer_transport.open()\n",
    "                    try:\n",
    "                        computer_transport.rmtree(remote_path)\n",
    "                    except IOError as ex:\n",
    "                        print(ex)\n",
    "    else:\n",
    "        print('Nothing to clean from the remote workdir!')\n",
    "# section-5:delete node from the database as well as remote work directory\n",
    "## It returns all the calcjob from a WC node\n",
    "def find_calcJob(pk_or_node, debug=True):\n",
    "    \n",
    "    calcjob_node_list=[]\n",
    "    wc_node_list = []\n",
    "    try:\n",
    "        if isinstance( pk_or_node, int):\n",
    "            if debug:\n",
    "                print('This is pk')\n",
    "            node = load_node(pk_or_node)\n",
    "        else:\n",
    "            if debug:\n",
    "                print('This is node.')\n",
    "            node= pk_or_node\n",
    "    except:\n",
    "        print('{} is nither node ID nor aiida_node. '.format(pk_or_node))\n",
    "        \n",
    "    ## Use the get_calcjob_wc to get descendent calcjob list and  wc list\n",
    "    calc_list, wc_list = get_calcjob_wc(node)\n",
    "    calcjob_node_list += calc_list\n",
    "    \n",
    "    while len(wc_list)!=0:\n",
    "        new_wc_list = []\n",
    "\n",
    "        for i in wc_list[:]:\n",
    "            calc_list, wc_list = get_calcjob_wc(i)\n",
    "            new_wc_list += wc_list\n",
    "            calcjob_node_list += calc_list\n",
    "            \n",
    "        wc_list = new_wc_list\n",
    "\n",
    "    return calcjob_node_list\n",
    "\n",
    "## This function returns calcjob_list and wc_list from a wc or calcjob node   \n",
    "def get_calcjob_wc(node):\n",
    "    \"\"\"\n",
    "    :param: node\n",
    "    :return: workchain node list and calcjob node list\n",
    "    \"\"\" \n",
    "    from aiida.orm import CalcJobNode, WorkChainNode\n",
    "    wc = []\n",
    "    calc_job = []\n",
    "    \n",
    "    if node.node_type == 'process.workflow.workchain.WorkChainNode.':\n",
    "        \n",
    "    # here all outgoing worchain node\n",
    "        out_going_wc = node.get_outgoing(node_class=WorkChainNode).all()\n",
    "        wc = [i.node for i in out_going_wc[:]]\n",
    "        \n",
    "    # here all outgoing calcjob node\n",
    "        out_going_calc = node.get_outgoing(node_class=CalcJobNode).all()\n",
    "        calc_job = [i.node for i in out_going_calc[:]]\n",
    "                    \n",
    "    elif node.node_type == 'process.calculation.calcjob.CalcJobNode.':\n",
    "        calc_job.append(node)\n",
    "    \n",
    "    return calc_job, wc\n",
    "\n",
    "# This is the final del_node_function. Using this function for any specific wc node the node from the \n",
    "# Db as well as the calcjob data from the remote workdir can be deleted.\n",
    "def del_node(node_pks, dry_run=True, verbosity=3, debug=True, only_remote_dir=False,\n",
    "            only_database=False):\n",
    "    \"\"\"\n",
    "    1. This function will delete the node data from the database and also from the remote_dir\n",
    "    \n",
    "    :params node_pks: (list) list of workchain to delete from database as well as from remote workdir\n",
    "    :param verbosity: 0 prints nothing.  This is for workdir and wc\n",
    "                      1 prints just sums and total.   This is for workdir but not for wc\n",
    "                      2 prints indivisual nodes.  This is for workdir and wc\n",
    "    :param dry_run: Do not delete anything just show the status as in the verbosity given\n",
    "    \"\"\"\n",
    "    from aiida.orm import load_node\n",
    "    from aiida.manage.database.delete.nodes import delete_nodes\n",
    "    \n",
    "        \n",
    "    calcjobs_list = []\n",
    "    pks_given = []\n",
    "    for i in node_pks:    \n",
    "        try:\n",
    "            if isinstance( i, int):\n",
    "                if debug:\n",
    "                    print('This might be pk or uiid')\n",
    "                node = load_node(i)\n",
    "            else:\n",
    "                if debug:\n",
    "                    print('This might be a node.')\n",
    "                node= i\n",
    "        except:\n",
    "            print('{} is nither node ID nor aiida_node. '.format(i))\n",
    "        \n",
    "        pks_given.append(node.pk)\n",
    "        \n",
    "        calcjobs = find_calcJob(node, debug)\n",
    "        calcjobs_list += calcjobs\n",
    "        print('calcjob list : ', calcjobs_list,)\n",
    "    if only_remote_dir:\n",
    "        delete_remote_workdir(calcjobs_list, verbosity=verbosity, dry_run=dry_run)\n",
    "    if only_database:\n",
    "        delete_nodes(pks_given, verbosity=verbosity, dry_run=dry_run,force=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "primary-tennis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One very frequently used option\n",
    "metadata_option_1 = {'max_wallclock_seconds': 36000,'resources': \n",
    "               {'tot_num_mpiprocs': 48, 'num_machines': 1},\n",
    "              'custom_scheduler_commands': \n",
    "              '#SBATCH --account=jara0191\\n\\nulimit -s unlimited; export OMP_STACKSIZE=2g',\n",
    "              'withmpi': True\n",
    "                    }\n",
    "oscar_matadata = {'max_wallclock_seconds': 8*60*60,\n",
    "                  'resources':{'tot_num_mpiprocs': 12, \n",
    "                  'num_machines': 4},\n",
    "                  'custom_scheduler_commands': '#SBATCH -p oscar\\n\\nulimit -s unlimited; export OMP_STACKSIZE= 2g',\n",
    "                  'withmpi':True\n",
    "                 }\n",
    "voro_code = Code.get_from_string('voro@claix18_init')\n",
    "kkr_code = Code.get_from_string('kkr@claix18_init')\n",
    "kkrimp_code = Code.get_from_string('kkrflex@claix18_init')\n",
    "iffslurm_voro = Code.get_from_string('voro@iffslurm')\n",
    "iffslurm_kkr= Code.get_from_string('kkr@iffslurm')\n",
    "iffslurm_kkrimp= Code.get_from_string('kkrflex@iffslurm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "conditional-money",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that here for doped 3d and 4d atoms into the Bi2Te3 host saved in group-74\n",
    "\n",
    "group_74 = load_group(74)\n",
    "## first node of group-74 is for kkr_imp_wc process\n",
    "# is being used for the host_gf or kkr_flex files\n",
    "\n",
    "imp_wc_with_gf = load_node(16100)\n",
    "imp1_wc = group_74.nodes[1]\n",
    "imp2_wc = group_74.nodes[2]\n",
    "\n",
    "# gf_host_remote = imp_wc_with_gf.inputs.remote_data_gf\n",
    "# host_gf_wc = imp_wc_with_gf.get_outgoing(node_class=kkr_flex_wc).all()[0].node\n",
    "\n",
    "\n",
    "impurity1_output_node = imp1_wc.outputs.workflow_info\n",
    "impurity2_output_node = imp2_wc.outputs.workflow_info\n",
    "offset_imp2 = {'index':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "vocational-harassment",
   "metadata": {},
   "outputs": [],
   "source": [
    "scf_node = imp1_wc.get_outgoing(node_class=kkr_imp_sub_wc).all()[0].node\n",
    "## kkr_flex_wc\n",
    "host_gf = imp1_wc.inputs.remote_data_gf.get_incoming(node_class=kkr_flex_wc).all()[0].node\n",
    "kkr_calc = host_gf.get_outgoing(node_class=KkrCalculation).all()[0].node\n",
    "kkr_flex_kkr_param = kkr_calc.inputs.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "unnecessary-ethics",
   "metadata": {},
   "outputs": [],
   "source": [
    "kkr_flex_kkr_param = kkr_calc.inputs.parameters.get_dict()\n",
    "kkr_flex_kkr_param['RCLUSTZ'] *= 1\n",
    "kkr_flex_kkr_param['NSHELD'] = 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "renewable-novelty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version of workflow: 0.3.1\n"
     ]
    }
   ],
   "source": [
    "settings = combine_imps_wc.get_wf_defaults()\n",
    "builder = combine_imps_wc.get_builder()\n",
    "settings['jij_run']= True\n",
    "settings['retrieve_kkrflex'] = False\n",
    "settings['mag_init']= False\n",
    "settings['threshold_switch_high_accuracy'] = 1e-2\n",
    "settings['convergence_criterion'] = 1e-7 #convergence_criterion\n",
    "settings['threshold_aggressive_mixing']= 0.05\n",
    "settings['strmix']= 0.01\n",
    "settings['aggrmix'] = 0.05\n",
    "settings['broyden-number'] = 20\n",
    "settings['nsteps'] = 100\n",
    "settings['kkr_runmax'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "accessory-universal",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = scf_node.inputs.options.get_dict()\n",
    "options['max_wallclock_seconds'] = 36000*1\n",
    "# options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "reliable-amazon",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder.impurity1_output_node = impurity1_output_node\n",
    "builder.impurity2_output_node = impurity2_output_node\n",
    "builder.offset_imp2 = Dict(dict=offset_imp2)\n",
    "\n",
    "# scf namespace setup\n",
    "builder.scf.kkrimp = scf_node.inputs.kkrimp\n",
    "builder.scf.options = Dict(dict=options)\n",
    "builder.scf.wf_parameters = scf_node.inputs.wf_parameters\n",
    "\n",
    "# host_gf namespace setup\n",
    "builder.host_gf.kkr = host_gf.inputs.kkr\n",
    "builder.host_gf.options = host_gf.inputs.options\n",
    "builder.host_gf.params_kkr_overwrite = Dict(dict=kkr_flex_kkr_param) #host_gf.inputs.wf_parameters\n",
    "\n",
    "builder.wf_parameters_overwrite = Dict(dict=settings)\n",
    "builder.metadata.label = str(imp1_wc.label+ ':' +imp2_wc.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "wooden-constraint",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To submit the builder\n",
    "# combined_imp_submission = submit(builder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "innovative-average",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version of workflow: 0.3.1\n",
      "label :  Sc:Mn:Bi2Te3_il_4_il_4\n"
     ]
    }
   ],
   "source": [
    "# Note that here for doped 3d atoms into the Bi2Te3 host\n",
    "\n",
    "group_74 = load_group(74)\n",
    "group_91 = load_group(91)\n",
    "\n",
    "## first node of group-64 is for kkr_imp_wc process\n",
    "# is being used for the host_gf or kkr_flex files\n",
    "\n",
    "imp1_wc = group_91.nodes[0]\n",
    "imp2_wc = group_91.nodes[1]\n",
    "\n",
    "scf = imp1_wc\n",
    "\n",
    "params_kkr_overwrite = Dict(dict={'NSHELD' : 2500})\n",
    "host_kkr = Code().get_from_string('kkr@claix18_init')\n",
    "\n",
    "scf_kkrimp = Code().get_from_string('kkrflex@claix18_init')\n",
    "options = Dict(dict=metadata_option_1)\n",
    "\n",
    "imp1_output_node = imp1_wc.outputs.workflow_info\n",
    "imp2_output_node = imp2_wc.outputs.workflow_info\n",
    "offset_imp2 = {'index':1}\n",
    "\n",
    "\n",
    "\n",
    "settings = combine_imps_wc.get_wf_defaults()\n",
    "settings['jij_run'] = True\n",
    "settings['dos_run'] = False\n",
    "settings['lmdos'] = False\n",
    "settings['strmix'] = 0.02\n",
    "settings['aggrmix'] = 0.05\n",
    "settings['threshold_aggressive_mixing'] = 0.05\n",
    "settings['convergence_criterion'] = 1e-7\n",
    "settings['threshold_switch_high_accuracy'] = 1e-2\n",
    "settings['retrieve_kkrflex'] = False\n",
    "settings['mag_init'] = False\n",
    "settings['mixreduce'] = 0.5\n",
    "settings['nsteps'] = 100\n",
    "settings['kkr_runmax'] = 10\n",
    "settings['broyden-number'] = 20\n",
    "\n",
    "\n",
    "\n",
    "# if this function works then delete the below section\n",
    "ilayer1 = imp1_wc.inputs.impurity_info.get_dict()['ilayer_center']\n",
    "ilayer2 = imp2_wc.inputs.impurity_info.get_dict()['ilayer_center']\n",
    "\n",
    "label = imp1_wc.label.split(':')[0] + ':'+imp2_wc.label + '_il_' +str(ilayer1) + '_il_'+str(ilayer2)\n",
    "print('label : ',label)\n",
    "# submit_combine_imp(imp1_wc_node=imp1_wc, imp2_wc_node=imp2_wc, kkr_code=host_kkr, kkr_imp_code=scf_kkrimp, offset_imp2=offset_imp2, \n",
    "#                    options = options, scf_wf_parameters=None, params_kkr_overwrite=params_kkr_overwrite,settings=settings, \n",
    "#                    gf_host_remote=None, dry_run=True, label=label\n",
    "#                   )'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intimate-dominican",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "regular-equation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version of workflow: 0.3.1\n"
     ]
    }
   ],
   "source": [
    "## Here trying to work with host Au and other impurity\n",
    "## THe calc is taken from the wasmer\n",
    "imp_Au_group = load_group(85)\n",
    "wc_list = list(imp_Au_group.nodes)\n",
    "\n",
    "\n",
    "imp_wc_1 = wc_list[0]\n",
    "# print('first imp wc : ', imp_wc_1.label)\n",
    "imp_wc_2 = wc_list[1]\n",
    "# print('econd imp wc : ', imp_wc_2.label)\n",
    "# setting the scf and host namespace\n",
    "scf = imp_wc_1\n",
    "\n",
    "host_wf_parameters = Dict(dict={'NSHELD' : 2500,\n",
    "                               })\n",
    "\n",
    "host_kkr = Code().get_from_string('kkr@claix18_init')\n",
    "scf_kkrimp = Code().get_from_string('kkrflex@claix18_init')\n",
    "options = Dict(dict=metadata_option_1)\n",
    "\n",
    "imp1_output_node = imp_wc_1.outputs.workflow_info\n",
    "imp2_output_node = imp_wc_2.outputs.workflow_info\n",
    "offset_imp2 = {'index':1}\n",
    "\n",
    "settings = combine_imps_wc.get_wf_defaults()\n",
    "settings['jij_run'] = False\n",
    "settings['lmdos'] = False\n",
    "settings['strmix'] = 0.001\n",
    "settings['aggrmix'] = 0.06\n",
    "settings['threshold_aggressive_mixing'] = 0.06\n",
    "settings['convergence_criterion'] = 1e-7\n",
    "settings['threshold_switch_high_accuracy'] = 1e-2\n",
    "settings['retrieve_kkrflex'] = False\n",
    "settings['mag_init'] = False\n",
    "settings['mixreduce'] = 0.5\n",
    "settings['nsteps'] = 100\n",
    "settings['kkr_runmax'] = 10\n",
    "\n",
    "combine_imp_builder = combine_imps_wc.get_builder()\n",
    "combine_imp_builder.impurity1_output_node = imp1_output_node\n",
    "combine_imp_builder.impurity2_output_node = imp2_output_node\n",
    "combine_imp_builder.offset_imp2 = Dict(dict=offset_imp2)\n",
    "\n",
    "# scf namespace setup\n",
    "combine_imp_builder.scf.kkrimp = scf_kkrimp\n",
    "combine_imp_builder.scf.options = options\n",
    "combine_imp_builder.scf.wf_parameters = scf.inputs.wf_parameters\n",
    "# combine_imp_builder.gf_host_remote\n",
    "# host_gf namespace setup\n",
    "combine_imp_builder.host_gf.kkr = host_kkr\n",
    "combine_imp_builder.host_gf.options = options\n",
    "combine_imp_builder.host_gf.params_kkr_overwrite = host_wf_parameters #host_gf.inputs.wf_parameters\n",
    "\n",
    "combine_imp_builder.wf_parameters_overwrite = Dict(dict=settings)\n",
    "combine_imp_builder.metadata.label = str(imp1_wc.label+ ':' +imp2_wc.label)\n",
    "# submission = submit(combine_imp_builder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "official-comment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Rcut': 4.643155, 'Zimp': 44, 'ilayer_center': 0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_wc_1.get_outgoing(node_class=kkr_imp_sub_wc).first().node.inputs.impurity_info.get_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "transsexual-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_74 = load_group(74)\n",
    "group_91 = load_group(91)\n",
    "group_74 = list(group_74.nodes)\n",
    "group_91 = list(group_91.nodes)\n",
    "# 3vs3 indx1\n",
    "# 3vs4 indx1\n",
    "###Code list\n",
    "kkr_code = Code.get_from_string('kkr@claix18_init')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "molecular-grave",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node_label-Sc:Sc:Bi2Te3_il_3_il_4 is exist in the group-3d_4d_dope_Bi2Te3_il_3:il_4_offset_1.\n",
      "node_label-Sc:Mn:Bi2Te3_il_3_il_4 is exist in the group-3d_4d_dope_Bi2Te3_il_3:il_4_offset_1.\n",
      "node_label-Sc:Cr:Bi2Te3_il_3_il_4 is exist in the group-3d_4d_dope_Bi2Te3_il_3:il_4_offset_1.\n",
      "node_label-Sc:V:Bi2Te3_il_3_il_4 is exist in the group-3d_4d_dope_Bi2Te3_il_3:il_4_offset_1.\n",
      "node_label-Sc:Ti:Bi2Te3_il_3_il_4 is exist in the group-3d_4d_dope_Bi2Te3_il_3:il_4_offset_1.\n",
      "node_label-Sc:Ni:Bi2Te3_il_3_il_4 is exist in the group-3d_4d_dope_Bi2Te3_il_3:il_4_offset_1.\n",
      "node_label-Sc:Fe:Bi2Te3_il_3_il_4 is exist in the group-3d_4d_dope_Bi2Te3_il_3:il_4_offset_1.\n",
      "node_label-Sc:Y:Bi2Te3_il_3_il_4 is exist in the group-3d_4d_dope_Bi2Te3_il_3:il_4_offset_1.\n",
      "node_label-Sc:Zn:Bi2Te3_il_3_il_4 is exist in the group-3d_4d_dope_Bi2Te3_il_3:il_4_offset_1.\n",
      "node_label-Sc:Zr:Bi2Te3_il_3_il_4 is exist in the group-3d_4d_dope_Bi2Te3_il_3:il_4_offset_1.\n",
      "node_label-Sc:Cu:Bi2Te3_il_3_il_4 is exist in the group-3d_4d_dope_Bi2Te3_il_3:il_4_offset_1.\n",
      "node_label-Sc:Nb:Bi2Te3_il_3_il_4 is exist in the group-3d_4d_dope_Bi2Te3_il_3:il_4_offset_1.\n",
      "node_label-Sc:Mo:Bi2Te3_il_3_il_4 is exist in the group-3d_4d_dope_Bi2Te3_il_3:il_4_offset_1.\n",
      "node_label-Sc:Tc:Bi2Te3_il_3_il_4 is exist in the group-3d_4d_dope_Bi2Te3_il_3:il_4_offset_1.\n",
      "node_label-Sc:Rh:Bi2Te3_il_3_il_4 is exist in the group-3d_4d_dope_Bi2Te3_il_3:il_4_offset_1.\n",
      "node_label-Sc:Cd:Bi2Te3_il_3_il_4 is exist in the group-3d_4d_dope_Bi2Te3_il_3:il_4_offset_1.\n",
      "node_label-Sc:Pd:Bi2Te3_il_3_il_4 is exist in the group-3d_4d_dope_Bi2Te3_il_3:il_4_offset_1.\n",
      "node_label-Sc:Ru:Bi2Te3_il_3_il_4 is exist in the group-3d_4d_dope_Bi2Te3_il_3:il_4_offset_1.\n",
      "node_label-Sc:Ag:Bi2Te3_il_3_il_4 is exist in the group-3d_4d_dope_Bi2Te3_il_3:il_4_offset_1.\n",
      "val :  {'node_truple': (<WorkChainNode: uuid: c42b89d5-4af1-4177-b030-d877cc8c7bd0 (pk: 16195) (aiida.workflows:kkr.imp)>, <WorkChainNode: uuid: 01dc2bfd-1d48-4a20-b3f8-c3e8a971a799 (pk: 62167) (aiida.workflows:kkr.imp)>), 'pk_truple': (16195, 62167), 'label': 'Sc:Co:Bi2Te3_il_3_il_4', 'submission': None}\n",
      "THe submitted combine Imps is : 62765\n",
      "all_failed_num : 1, last pk : 62765\n",
      "This is while loop break, as all_submission_dict.\n",
      "node_label-Ti:Sc:Bi2Te3_il_3_il_4 is exist in the group-3d_4d_dope_Bi2Te3_il_3:il_4_offset_1.\n",
      "node_label-Ti:Mn:Bi2Te3_il_3_il_4 is exist in the group-3d_4d_dope_Bi2Te3_il_3:il_4_offset_1.\n",
      "node_label-Ti:Cr:Bi2Te3_il_3_il_4 is exist in the group-3d_4d_dope_Bi2Te3_il_3:il_4_offset_1.\n",
      "node_label-Ti:V:Bi2Te3_il_3_il_4 is exist in the group-3d_4d_dope_Bi2Te3_il_3:il_4_offset_1.\n",
      "node_label-Ti:Ti:Bi2Te3_il_3_il_4 is exist in the group-3d_4d_dope_Bi2Te3_il_3:il_4_offset_1.\n",
      "node_label-Ti:Ni:Bi2Te3_il_3_il_4 is exist in the group-3d_4d_dope_Bi2Te3_il_3:il_4_offset_1.\n",
      "node_label-Ti:Fe:Bi2Te3_il_3_il_4 is exist in the group-3d_4d_dope_Bi2Te3_il_3:il_4_offset_1.\n",
      "node_label-Ti:Y:Bi2Te3_il_3_il_4 is exist in the group-3d_4d_dope_Bi2Te3_il_3:il_4_offset_1.\n",
      "node_label-Ti:Zn:Bi2Te3_il_3_il_4 is exist in the group-3d_4d_dope_Bi2Te3_il_3:il_4_offset_1.\n",
      "node_label-Ti:Zr:Bi2Te3_il_3_il_4 is exist in the group-3d_4d_dope_Bi2Te3_il_3:il_4_offset_1.\n",
      "node_label-Ti:Cu:Bi2Te3_il_3_il_4 is exist in the group-3d_4d_dope_Bi2Te3_il_3:il_4_offset_1.\n",
      "node_label-Ti:Nb:Bi2Te3_il_3_il_4 is exist in the group-3d_4d_dope_Bi2Te3_il_3:il_4_offset_1.\n",
      "node_label-Ti:Mo:Bi2Te3_il_3_il_4 is exist in the group-3d_4d_dope_Bi2Te3_il_3:il_4_offset_1.\n",
      "node_label-Ti:Tc:Bi2Te3_il_3_il_4 is exist in the group-3d_4d_dope_Bi2Te3_il_3:il_4_offset_1.\n",
      "node_label-Ti:Rh:Bi2Te3_il_3_il_4 is exist in the group-3d_4d_dope_Bi2Te3_il_3:il_4_offset_1.\n",
      "node_label-Ti:Cd:Bi2Te3_il_3_il_4 is exist in the group-3d_4d_dope_Bi2Te3_il_3:il_4_offset_1.\n",
      "node_label-Ti:Pd:Bi2Te3_il_3_il_4 is exist in the group-3d_4d_dope_Bi2Te3_il_3:il_4_offset_1.\n",
      "node_label-Ti:Ru:Bi2Te3_il_3_il_4 is exist in the group-3d_4d_dope_Bi2Te3_il_3:il_4_offset_1.\n",
      "node_label-Ti:Ag:Bi2Te3_il_3_il_4 is exist in the group-3d_4d_dope_Bi2Te3_il_3:il_4_offset_1.\n",
      "val :  {'node_truple': (<WorkChainNode: uuid: 38097eba-5ead-4e14-be9f-72d534638c47 (pk: 16215) (aiida.workflows:kkr.imp)>, <WorkChainNode: uuid: 01dc2bfd-1d48-4a20-b3f8-c3e8a971a799 (pk: 62167) (aiida.workflows:kkr.imp)>), 'pk_truple': (16215, 62167), 'label': 'Ti:Co:Bi2Te3_il_3_il_4', 'submission': None}\n",
      "This is for loop break, as all_failed_num == max_fail_wc\n",
      "total failed or sucsess wc :  38\n"
     ]
    }
   ],
   "source": [
    "# Prepartion to submit il-3 Vs il-3 for the 3d_&_4d block\n",
    "\n",
    "# extract from parent wc\n",
    "## node 24418 is succefull wc for li-3 il-3 from node 16215 and 16237\n",
    "## node 24493 is succefull wc for li-4 il-4 from node 18853 and 18853\n",
    "## node 24510 is succefull wc for li-4 il-4 from node 18853 and 16215\n",
    "\n",
    "#### This part from group 74 and 91 ----------------------------\n",
    "parent_wc = load_node(24830)\n",
    "## This part for the failed combined calculations by increasing the BZDIVIDE-------------\n",
    "params_kkr_overwrite = {'BZDIVIDE': [60,60,60],\n",
    "                        'strmix': 0.001,\n",
    "                        'NSHELD': 2500\n",
    "                       }\n",
    "## ------------------------------------------\n",
    "kkr_imp_code = parent_wc.inputs.scf__kkrimp\n",
    "options = parent_wc.inputs.scf__options\n",
    "scf_wf_parameters = parent_wc.inputs.scf__wf_parameters\n",
    "\n",
    "settings = parent_wc.inputs.wf_parameters_overwrite\n",
    "## If need use the customised settings\n",
    "settings_customised = {}\n",
    "settings_customised['jij_run'] = False\n",
    "settings_customised['lmdos'] = False\n",
    "settings_customised['strmix'] = 0.001\n",
    "settings_customised['aggrmix'] = 0.06\n",
    "settings_customised['threshold_aggressive_mixing'] = 0.01\n",
    "settings_customised['convergence_criterion'] = 1e-7\n",
    "settings_customised['threshold_switch_high_accuracy'] = 1e-2\n",
    "settings_customised['retrieve_kkrflex'] = False\n",
    "settings_customised['mag_init'] = False\n",
    "settings_customised['mixreduce'] = 0.5\n",
    "settings_customised['nsteps'] = 100\n",
    "settings_customised['kkr_runmax'] = 10\n",
    "\n",
    "gf_host_remote = parent_wc.outputs.remote_data_gf\n",
    "\n",
    "offset_imp2 = parent_wc.inputs.offset_imp2\n",
    "\n",
    "si_imp_list1 = group_74\n",
    "si_imp_list2 = group_91\n",
    "\n",
    "succ_group_label = '3d_4d_dope_Bi2Te3_il_3:il_4_offset_1'\n",
    "succ_group_descr = 'only from group ID 74 and 91'\n",
    "\n",
    "submit_double_imp_wc_bunch( si_imp_list1= si_imp_list1, \n",
    "                            si_imp_list2= si_imp_list2, \n",
    "                            kkr_code= kkr_code, \n",
    "                            kkr_imp_code= kkr_imp_code, \n",
    "                            builder_options= options,  \n",
    "                            succ_group_label= succ_group_label,\n",
    "                            succ_group_descr= succ_group_descr,\n",
    "                            offset_imp2= offset_imp2,\n",
    "                            max_submission= 1,\n",
    "                            setting= settings, \n",
    "#                             gf_host_remote= gf_host_remote,\n",
    "                            scf_wf_parameters= scf_wf_parameters, \n",
    "                            params_kkr_overwrite= Dict(dict=params_kkr_overwrite),\n",
    "                            dry_run= False,\n",
    "                            max_fail_wc= 1,\n",
    "                            debug= False\n",
    "                              )\n",
    "#### This part from group 74 and 91 ----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "played-azerbaijan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepartion to submit il-3 Vs il-3 for the 3d_&_4d block\n",
    "\n",
    "# extract from parent wc\n",
    "## node 24418 is succefull wc for li-3 il-3 from node 16215 and 16237\n",
    "## node 24493 is succefull wc for li-4 il-4 from node 18853 and 18853\n",
    "## node 24510 is succefull wc for li-4 il-4 from node 18853 and 16215\n",
    "\n",
    "#### This part from group 74 and 91 ----------------------------\n",
    "parent_wc = load_node(24887)\n",
    "\n",
    "kkr_imp_code = parent_wc.inputs.scf__kkrimp\n",
    "options = parent_wc.inputs.scf__options\n",
    "scf_wf_parameters = parent_wc.inputs.scf__wf_parameters\n",
    "\n",
    "settings = parent_wc.inputs.wf_parameters_overwrite\n",
    "\n",
    "gf_host_remote = parent_wc.outputs.remote_data_gf\n",
    "# params_kkr_overwrite = parent_wc.\n",
    "\n",
    "offset_imp2 = parent_wc.inputs.offset_imp2\n",
    "\n",
    "si_imp_list1 = group_74\n",
    "si_imp_list2 = group_74\n",
    "\n",
    "succ_group_label = '3d_4d_dope_Bi2Te3_il_3:il_3_offset_1'\n",
    "succ_group_descr = 'only from group ID 74 and 74'\n",
    "\n",
    "# submit_double_imp_wc_bunch( si_imp_list1= si_imp_list1, \n",
    "#                             si_imp_list2= si_imp_list2, \n",
    "#                             kkr_code= None, \n",
    "#                             kkr_imp_code= kkr_imp_code, \n",
    "#                             builder_options= options,  \n",
    "#                             succ_group_label= succ_group_label,\n",
    "#                             succ_group_descr= succ_group_descr,\n",
    "#                             offset_imp2= offset_imp2,\n",
    "#                             max_submission= 20,\n",
    "#                             setting= settings_customised, \n",
    "#                             gf_host_remote= gf_host_remote,\n",
    "#                             scf_wf_parameters= scf_wf_parameters, \n",
    "#                             params_kkr_overwrite= None,\n",
    "#                             dry_run= False,\n",
    "#                             max_fail_wc= 40,\n",
    "#                             debug= False\n",
    "#                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "great-vermont",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To submit the only failed calculation\n",
    "calc_25059 = load_node(25059)\n",
    "rebuilder = calc_25059.get_builder_restart()\n",
    "# resubmit = submit(rebuilder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "naughty-signature",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-c4731fdceed0>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-c4731fdceed0>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    from ../aiida-jutools/aiida_jutools import utils\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from ../aiida-jutools/aiida_jutools import utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "polar-movement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/mozumder/JupyterHub/Daily_Work_bin'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "broadband-journalist",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'python_util' from 'masci_tools.util' (/opt/masci-tools/masci_tools/util/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b0b2a926d17f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmasci_tools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpython_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"quota\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mheader_line_count\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcolumn_space_used\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"used\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'python_util' from 'masci_tools.util' (/opt/masci-tools/masci_tools/util/__init__.py)"
     ]
    }
   ],
   "source": [
    "from masci_tools.util import python_util\n",
    "\n",
    "command: str = \"quota\"\n",
    "header_line_count: int = 1\n",
    "column_space_used: str = \"used\"\n",
    "column_space_hard: str = \"hard\"\n",
    "dirname_pattern: str = \"\"\n",
    "min_free_space: str = \"10G\"\n",
    "comment: List[AnyStr] = python_util.dataclass_default_field([\"\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-structure",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note: \n",
    "# with no 'gf_host_remote', 'jij_data', 'jij_info' NODE: 26881\n",
    "# with no  'jij_data', 'jij_info' NODE: 26903\n",
    "# with 'gf_host_remote', 'jij_data', 'jij_info' NODE: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "environmental-colors",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calcjob list :  [<CalcJobNode: uuid: ca120c95-2156-44d0-bc8a-c7f0d7678a37 (pk: 24873) (aiida.calculations:kkr.kkrimp)>, <CalcJobNode: uuid: bb949319-b607-4982-ba8a-5aae21f0d6a0 (pk: 24841) (aiida.calculations:kkr.kkr)>, <CalcJobNode: uuid: 87f8c5f6-9bad-48ff-94e9-74e294c81b14 (pk: 24856) (aiida.calculations:kkr.kkrimp)>, <CalcJobNode: uuid: 623afab6-41d7-4405-8260-c9dc0ec5478c (pk: 24863) (aiida.calculations:kkr.kkrimp)>]\n",
      "This is calcjob node\n",
      "This is calcjob node\n",
      "This is calcjob node\n",
      "This is calcjob node\n",
      "claix18_init\n",
      "claix18_init\n",
      "claix18_init\n",
      "claix18_init\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepting-dividend",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_104 = load_group(104).nodes\n",
    "node_pk = [node.pk for node in group_104]\n",
    "# del_node(node_pks= node_pk[:], dry_run=False, verbosity=0, debug=False, only_remote_dir=True, only_database=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "antique-journalist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i,j : (0, 1)\n",
      "i,j : (0, 2)\n",
      "i,j : (0, 3)\n",
      "i,j : (0, 4)\n",
      "i,j : (0, 5)\n",
      "i,j : (0, 6)\n",
      "i,j : (0, 7)\n",
      "i,j : (0, 8)\n",
      "i,j : (0, 9)\n",
      "i,j : (1, 2)\n",
      "i,j : (1, 3)\n",
      "i,j : (1, 4)\n",
      "i,j : (1, 5)\n",
      "i,j : (1, 6)\n",
      "i,j : (1, 7)\n",
      "i,j : (1, 8)\n",
      "i,j : (1, 9)\n",
      "i,j : (2, 3)\n",
      "i,j : (2, 4)\n",
      "i,j : (2, 5)\n",
      "i,j : (2, 6)\n",
      "i,j : (2, 7)\n",
      "i,j : (2, 8)\n",
      "i,j : (2, 9)\n",
      "i,j : (3, 4)\n",
      "i,j : (3, 5)\n",
      "i,j : (3, 6)\n",
      "i,j : (3, 7)\n",
      "i,j : (3, 8)\n",
      "i,j : (3, 9)\n",
      "i,j : (4, 5)\n",
      "i,j : (4, 6)\n",
      "i,j : (4, 7)\n",
      "i,j : (4, 8)\n",
      "i,j : (4, 9)\n",
      "i,j : (5, 6)\n",
      "i,j : (5, 7)\n",
      "i,j : (5, 8)\n",
      "i,j : (5, 9)\n",
      "i,j : (6, 7)\n",
      "i,j : (6, 8)\n",
      "i,j : (6, 9)\n",
      "i,j : (7, 8)\n",
      "i,j : (7, 9)\n",
      "i,j : (8, 9)\n"
     ]
    }
   ],
   "source": [
    "N = 10\n",
    "for i in range(N-1):\n",
    "    for j in range(N)[i+1:]:\n",
    "        print(f'i,j : {i,j}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-professional",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bunch_wc_submission_package.submit_bunch import bunch_wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocational-mistress",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AiiDA",
   "language": "python",
   "name": "aiida"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
