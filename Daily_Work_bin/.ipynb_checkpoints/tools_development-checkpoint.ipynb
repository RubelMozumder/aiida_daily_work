{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "banned-lindsay",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "profile name :  default\n",
      "profile database host name :  localhost\n",
      "default user :  r.mozumder@fz-juelich.de\n"
     ]
    }
   ],
   "source": [
    "#load aiida environment and connect to database\n",
    "from aiida import load_profile\n",
    "from aiida.orm import computers\n",
    "profile = load_profile()\n",
    "print('profile name : ',profile.name)\n",
    "print('profile database host name : ',profile.database_hostname)\n",
    "print('default user : ',profile.default_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "related-penalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load classes and functions\n",
    "from aiida.orm import CifData, Code, Dict, load_node, StructureData \n",
    "from aiida.orm import (Group, load_group, load_node, load_code, groups,\n",
    "                      WorkChainNode, QueryBuilder)\n",
    "from aiida.engine import submit\n",
    "from aiida.common.exceptions import NotExistent\n",
    "from aiida_kkr.workflows import kkr_imp_sub_wc, kkr_imp_dos, kkr_imp_wc, kkr_startpot_wc\n",
    "import numpy as np\n",
    "from aiida_kkr.calculations import KkrimpCalculation, VoronoiCalculation\n",
    "import matplotlib.pyplot as plt\n",
    "from aiida_kkr.tools.common_workfunctions import get_username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "undefined-comedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure data from voronoi calc or kkr_startpot_wc\n",
    "def Struc_from_voro(voro_node, host_structure=True, imp_structure=False):\n",
    "    # Packages and module\n",
    "    from aiida.common.exceptions import InputValidationError\n",
    "    from aiida_kkr.calculations import VoronoiCalculation\n",
    "    from aiida_kkr.workflows import kkr_startpot_wc \n",
    "    from aiida.orm import StructureData, Node\n",
    "    \"\"\"\n",
    "    Structure from the voronoi calcjob or voro_startpot_wc\n",
    "    voro_node: (voro_calc)\n",
    "    \"\"\"\n",
    "    # To check voro_node in node or ID\n",
    "    if not isinstance(voro_node, Node):\n",
    "        try:\n",
    "            voro_node = load_node(voro_node)\n",
    "        except IndentationError:\n",
    "            print('No such pk or uuid: {} is present in the database.'.format(voro_node))\n",
    "            \n",
    "    host_struc, imp_host_struc = None, None\n",
    "    \n",
    "    # For voronoi calcjob\n",
    "    if (voro_node.process_class==VoronoiCalculation) :\n",
    "        ## Degug here\n",
    "        print('This Voronoicalculation')\n",
    "        struc = voro_node.inputs.structure\n",
    "        # Check for is it from ancector wc or created there\n",
    "        # For example the struc from sub kkr_startpot_wc in kkr_imp_wc is created there so \n",
    "        # no inconing_node is available \n",
    "        \n",
    "        if struc.get_incoming().first() == []:\n",
    "            imp_host_struc = struc\n",
    "    # For kkr_startpot_wc  \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-relief",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "raising-forum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group as named imp_embedded_Au_by_wasmer is already stored, so no need to create the group\n",
      "node 16832 is already exist\n",
      "node 16805 is already exist\n",
      "node 16936 is already exist\n",
      "node 16982 is already exist\n",
      "node 16986 is already exist\n",
      "node 16996 is already exist\n",
      "node 17068 is already exist\n",
      "node 17141 is already exist\n",
      "node 17301 is already exist\n",
      "node 17363 is already exist\n",
      "node 17442 is already exist\n",
      "node 17477 is already exist\n",
      "node 17532 is already exist\n",
      "node 17588 is already exist\n",
      "node 17653 is already exist\n",
      "node 17659 is already exist\n",
      "node 17735 is already exist\n",
      "node 17746 is already exist\n",
      "node 17751 is already exist\n",
      "node 17787 is already exist\n",
      "node 17826 is already exist\n",
      "node 17919 is already exist\n",
      "node 17948 is already exist\n",
      "node 17986 is already exist\n",
      "node 18166 is already exist\n",
      "node 18207 is already exist\n",
      "node 18210 is already exist\n",
      "node 18237 is already exist\n",
      "node 18287 is already exist\n",
      "node 18474 is already exist\n",
      "node 18539 is already exist\n",
      "node 18547 is already exist\n",
      "node 18590 is already exist\n",
      "node 18592 is already exist\n",
      "node 18638 is already exist\n",
      "node 18698 is already exist\n",
      "node 18759 is already exist\n",
      "node 18775 is already exist\n",
      "node 18798 is already exist\n"
     ]
    }
   ],
   "source": [
    "####### NOTE: If this csucessful then add it into the 'tools_development' ipynb\n",
    "## In this part the nodes from a group having with different class and kind of nodes, \n",
    "## can be collected to store in new group only considering the kkr_imp_wc node\n",
    "\n",
    "group_wasmer = load_group(83)\n",
    "# node_label\n",
    "debug= False\n",
    "wasmer_grp_list = list(group_wasmer.nodes)\n",
    "group_label = 'imp_embedded_Au_by_wasmer'\n",
    "try:\n",
    "    new_grp = load_group(group_label)\n",
    "    new_grp_list = list(new_grp.nodes)\n",
    "    new_grp_list =  [node.pk for node in new_grp_list]\n",
    "    print('Group as named %s is already stored, so no need to create the group'%group_label)\n",
    "except:\n",
    "    new_grp = Group(label=group_label)\n",
    "    new_grp.store()\n",
    "    print(' A new node created as named %s' %node_label)\n",
    "\n",
    "i=0\n",
    "imp_wc_no = 0\n",
    "for index in range(len(wasmer_grp_list)):\n",
    "    node = wasmer_grp_list[index]\n",
    "    if node.node_type.split('.')[-2]=='WorkChainNode':\n",
    "        if (u'kkr_imp_wc' == node.process_label):\n",
    "            imp_wc_no += 1\n",
    "            if node.pk in new_grp_list[:]:\n",
    "                print('node {} is already exist'.format(node.pk))\n",
    "            else:\n",
    "                new_grp.add_nodes(node)\n",
    "#             if debug:\n",
    "#                 print(node.process_label)\n",
    "#                 print('pk: %7d'%node.pk)\n",
    "#                 print(wasmer_grp_list[i])\n",
    "#                 print(node.label)\n",
    "                \n",
    "                        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "consecutive-paint",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Trying to develop to delete the node from database\n",
    "## and at the same time from the remote folder\n",
    "\n",
    "## Add the following possible services\n",
    "# 1. Give the print option for how many decendant node will be \n",
    "#       and take the permission. Add option wether need to take \n",
    "#         permission or not.\n",
    "# 2. Also print how many calcjob node will be deleted under permission \n",
    "# 3. Print'remote directory' to check that all the data from the\n",
    "#     remote dir are deleted or not.\n",
    "## Technique\n",
    "# 1. Use the QuaryDB()\n",
    "# Use the cleandir\n",
    "\n",
    "\n",
    "\n",
    "#Section-1: delete node from the database as well as remote work directory\n",
    "### A function to delete the data of calculation output of calcjob list (pks) from the remote dir.\n",
    "\n",
    "# later add it with the del_node  function\n",
    "# This is successfully done\n",
    "\n",
    "# please note that it is tested for one calc list\n",
    "def delete_remote_workdir(pks, verbosity=0, dry_run= True):\n",
    "    from aiida.common import exceptions\n",
    "    from aiida.orm import load_node\n",
    "    from aiida.orm import computers\n",
    "    import sys\n",
    "    # TODO : add the verbosity as discused here \n",
    "    # https://aiida.readthedocs.io/projects/aiida-core/en/v1.5.0/_modules/aiida/manage/database/delete/nodes.html\n",
    "\n",
    "    \"\"\"\n",
    "    :param pks: calc node list\n",
    "    \n",
    "    \"\"\"\n",
    "    removed_path_list = [] # The part of the path will be deleted\n",
    "    remote_path_list = []  # The original path\n",
    "    updated_path_list = [] # After removing the part of the path\n",
    "    loadable_list = [] # To load the node and save it loadable_list\n",
    "    loaded_node_list = []\n",
    "    # To check the loadable calcjob list\n",
    "    for pk in pks:\n",
    "        try:\n",
    "            loaded_node = load_node(pk)\n",
    "        except exceptions.NotExistent:\n",
    "            print('This is calcjob node'.format(pk))\n",
    "            loaded_node = pk\n",
    "            loaded_node_list.append(loaded_node)\n",
    "            #             sys.exit()\n",
    "        else:\n",
    "            loaded_node_list.append(loaded_node)\n",
    "    # Computer data\n",
    "    \n",
    "   \n",
    "    for node in loaded_node_list:\n",
    "        load_pk = node\n",
    "        # computer data\n",
    "        computer = load_pk.computer\n",
    "        computer_name = computer.label\n",
    "        print(computer_name)\n",
    "        \n",
    "        remote_path = load_pk.get_remote_workdir()\n",
    "        remote_path_list.append(remote_path)\n",
    "        \n",
    "        delete_folder = remote_path.split('/')[-1]\n",
    "        removed_path_list.append(delete_folder)\n",
    "\n",
    "        new_remote_path = remote_path.replace(remote_path.split('/')[-1], '')\n",
    "        updated_path_list.append(new_remote_path)\n",
    "        \n",
    "        \n",
    "        \n",
    "    if dry_run or verbosity==3:\n",
    "\n",
    "        for i, paths in enumerate(zip(remote_path_list, updated_path_list)):\n",
    "            print('Before the delation the original path list : {}\\n'.format(paths[0]))\n",
    "            print('After deletion the modefied or updated path : {}'.format(paths[1]))\n",
    "    val = input(\"Are you agree to clean the remote workdir (y/n) : \")\n",
    "    \n",
    "    if str(val)=='y' or str(val)=='Y':\n",
    "        for remote_path in remote_path_list:\n",
    "            if not dry_run:\n",
    "                    # Open the connection to the remote folder/dir via transport\n",
    "                    computer_transport = computer.get_transport()\n",
    "                    is_transport_open = computer_transport.is_open\n",
    "                    if not is_transport_open:\n",
    "                        computer_transport.open()\n",
    "                    try:\n",
    "                        computer_transport.rmtree(remote_path)\n",
    "                    except IOError as ex:\n",
    "                        print(ex)\n",
    "    else:\n",
    "        print('Nothing to clean from the remote workdir!')\n",
    "# section-5:delete node from the database as well as remote work directory\n",
    "## It returns all the calcjob from a WC node\n",
    "def find_calcJob(pk_or_node, debug=True):\n",
    "    \n",
    "    calcjob_node_list=[]\n",
    "    wc_node_list = []\n",
    "    try:\n",
    "        if isinstance( pk_or_node, int):\n",
    "            if debug:\n",
    "                print('This is pk')\n",
    "            node = load_node(pk_or_node)\n",
    "        else:\n",
    "            if debug:\n",
    "                print('This is node.')\n",
    "            node= pk_or_node\n",
    "    except:\n",
    "        print('{} is nither node ID nor aiida_node. '.format(pk_or_node))\n",
    "        \n",
    "    ## Use the get_calcjob_wc to get descendent calcjob list and  wc list\n",
    "    calc_list, wc_list = get_calcjob_wc(node)\n",
    "    calcjob_node_list += calc_list\n",
    "    \n",
    "    while len(wc_list)!=0:\n",
    "        new_wc_list = []\n",
    "\n",
    "        for i in wc_list[:]:\n",
    "            calc_list, wc_list = get_calcjob_wc(i)\n",
    "            new_wc_list += wc_list\n",
    "            calcjob_node_list += calc_list\n",
    "            \n",
    "        wc_list = new_wc_list\n",
    "\n",
    "    return calcjob_node_list\n",
    "\n",
    "## This function returns calcjob_list and wc_list from a wc or calcjob node   \n",
    "def get_calcjob_wc(node):\n",
    "    \"\"\"\n",
    "    :param: node\n",
    "    :return: workchain node list and calcjob node list\n",
    "    \"\"\" \n",
    "    from aiida.orm import CalcJobNode, WorkChainNode\n",
    "    wc = []\n",
    "    calc_job = []\n",
    "    \n",
    "    if node.node_type == 'process.workflow.workchain.WorkChainNode.':\n",
    "        \n",
    "    # here all outgoing worchain node\n",
    "        out_going_wc = node.get_outgoing(node_class=WorkChainNode).all()\n",
    "        wc = [i.node for i in out_going_wc[:]]\n",
    "        \n",
    "    # here all outgoing calcjob node\n",
    "        out_going_calc = node.get_outgoing(node_class=CalcJobNode).all()\n",
    "        calc_job = [i.node for i in out_going_calc[:]]\n",
    "                    \n",
    "    elif node.node_type == 'process.calculation.calcjob.CalcJobNode.':\n",
    "        calc_job.append(node)\n",
    "    \n",
    "    return calc_job, wc\n",
    "\n",
    "# This is the final del_node_function. Using this function for any specific wc node the node from the \n",
    "# Db as well as the calcjob data from the remote workdir can be deleted.\n",
    "def del_nodes(node_pks, dry_run=True, verbosity=3, debug=True, only_workdir= False,\n",
    "             only_database= False):\n",
    "    \"\"\"\n",
    "    1. This function will delete the node data from the database and also from the remote_dir\n",
    "    \n",
    "    :params node_pks: (list) list of workchain to delete from database as well as from remote workdir\n",
    "    :param verbosity: 0 prints nothing.  This is for workdir and wc\n",
    "                      1 prints just sums and total.   This is for workdir but not for wc\n",
    "                      2 prints indivisual nodes.  This is for workdir and wc\n",
    "    :param dry_run: Do not delete anything just show the status as in the verbosity given\n",
    "    \"\"\"\n",
    "    from aiida.orm import load_node\n",
    "    from aiida.manage.database.delete.nodes import delete_nodes\n",
    "        \n",
    "    calcjobs_list = []\n",
    "    pks_given = []\n",
    "    for i in node_pks:    \n",
    "        try:\n",
    "            if isinstance( i, int):\n",
    "                if debug:\n",
    "                    print('This might be pk or uiid')\n",
    "                node = load_node(i)\n",
    "            else:\n",
    "                if debug:\n",
    "                    print('This might be a node.')\n",
    "                node= i\n",
    "        except:\n",
    "            print('{} is nither node ID nor aiida_node. '.format(i))\n",
    "        \n",
    "        pks_given.append(node.pk)\n",
    "        \n",
    "        calcjobs = find_calcJob(node, debug)\n",
    "        calcjobs_list += calcjobs\n",
    "        print('calcjob list : ', calcjobs_list,)\n",
    "    if only_workdir:\n",
    "        delete_remote_workdir(calcjobs_list, verbosity=verbosity, dry_run=dry_run)\n",
    "    if only_database:\n",
    "        delete_nodes(pks_given, verbosity=verbosity, dry_run=dry_run,force=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beautiful-compromise",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "brave-dylan",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_107 = load_group(108)\n",
    "node_list = list(group_107.nodes)\n",
    "del_nodes(node_pks=node_list, dry_run= True, verbosity=3, debug= False, \n",
    "          only_workdir= False, only_database= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fundamental-environment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<CalcJobNode: uuid: 4f96b1e1-7bf9-442e-8a50-5147b733e794 (pk: 24667) (aiida.calculations:kkr.kkrimp)>,\n",
       " <CalcJobNode: uuid: b2b07931-8919-4216-ab1d-179be5f02979 (pk: 24635) (aiida.calculations:kkr.kkr)>,\n",
       " <CalcJobNode: uuid: 8cbbabd5-8288-469b-a045-9569e8a14876 (pk: 24650) (aiida.calculations:kkr.kkrimp)>,\n",
       " <CalcJobNode: uuid: 79ae3a10-9c5d-4b40-b44b-37bb294c9f81 (pk: 24657) (aiida.calculations:kkr.kkrimp)>]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "prescribed-grave",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "claix18_init\n",
      "claix18_init\n",
      "Before the delation the original path list : /rwthfs/rz/cluster/work/jara0191/ck142666/aiida/computers/claix18_init/8c/bb/abd5-8288-469b-a045-9569e8a14876\n",
      "\n",
      "After deletion the modefied or updated path : /rwthfs/rz/cluster/work/jara0191/ck142666/aiida/computers/claix18_init/8c/bb/\n",
      "Before the delation the original path list : /rwthfs/rz/cluster/work/jara0191/ck142666/aiida/computers/claix18_init/79/ae/3a10-9c5d-4b40-b44b-37bb294c9f81\n",
      "\n",
      "After deletion the modefied or updated path : /rwthfs/rz/cluster/work/jara0191/ck142666/aiida/computers/claix18_init/79/ae/\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Are you agree to clean the remote workdir (y/n) :  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nothing to clean from the remote workdir!\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-cannon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "nasty-corrections",
   "metadata": {},
   "outputs": [],
   "source": [
    "#section-2: delete node from the database as well as remote work directory\n",
    "## This code  will help to workchain as explained in the filter\n",
    "qb = QueryBuilder()\n",
    "node_list = list(qb.append([WorkChainNode],\n",
    "                    filters={\n",
    "                        'and':[\n",
    "                            {'attributes.process_label':'combine_imps_wc'},\n",
    "                            {'or':[\n",
    "                                {'attributes.process_status': 'excepted'},\n",
    "                                {'attributes.exit_status':{'!in':[0]}},\n",
    "                                {'attributes.process_status': 'killed'},\n",
    "                                \n",
    "                                 ]\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                   \n",
    "                   \n",
    "                   ).all())\n",
    "\n",
    "# node_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "corrected-technician",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "## section-3 : delete node from the database as well as remote work directory\n",
    "## To check any specific pk in the pk_list exist or not\n",
    "def check_pk_exist(pk_list, pk):\n",
    "    for i in pk:\n",
    "        for j in pk_list:\n",
    "            if(i==j):\n",
    "                print('pk-{} is exist in pk_list'.format(i))\n",
    "                continue\n",
    "\n",
    "len(node_list)\n",
    "\n",
    "pk_list = [i[0].pk for i in node_list]\n",
    "pk_list.sort()\n",
    "print(check_pk_exist(pk_list,[22232]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "involved-mississippi",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indian-study",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-tuition",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conceptual-regression",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AiiDA",
   "language": "python",
   "name": "aiida"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
